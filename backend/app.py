"""
í•˜ìˆ˜ë„ ë§‰í˜ ì‹¤ì‹œê°„ ê°ì§€ FastAPI ë°±ì—”ë“œ
ì„œë²„ì™€ í”„ë¡ íŠ¸ì—”ë“œ ë¶„ë¦¬ ë²„ì „
"""

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from collections import deque
from datetime import datetime, timedelta
import asyncio
import json
import logging
import math
import cv2
import base64
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="í•˜ìˆ˜ë„ ë§‰í˜ ê°ì§€ ì‹œìŠ¤í…œ API", version="2.0.0")

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ë¶„ë¦¬ë¡œ ì¸í•´ í•„ìš”)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš© - ì‹¤ì œ ë°°í¬ì‹œì—ëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== ë°ì´í„° ëª¨ë¸ ====================

class DetectionData(BaseModel):
    timestamp: str
    garbage_type: str
    confidence: float
    bbox: List[int]  # [x1, y1, x2, y2]
    area: float
    location: str = "main_pipe"

class AlertData(BaseModel):
    level: str  # "safe", "warning", "danger"
    message: str
    timestamp: datetime
    risk_score: float

class StatusResponse(BaseModel):
    risk_level: str
    risk_score: float
    total_detections: int
    last_detection: Optional[str]
    alerts_today: int
    pipe_status: str
    blockage_percentage: Optional[float] = 0.0
    garbage_volume: Optional[float] = 0.0
    flow_restriction: Optional[str] = "ì—†ìŒ"
    accumulated_areas: Optional[int] = 0

class BlockageAnalysis(BaseModel):
    blockage_percentage: float
    garbage_volume: float
    flow_restriction: str
    accumulated_areas: int
    total_area: float

class AIAnalysis(BaseModel):
    """AI ë¶„ì„ ê²°ê³¼"""
    risk_assessment: str  # "low", "medium", "high", "critical"
    confidence_level: float  # AI ë¶„ì„ ì‹ ë¢°ë„ (0-1)
    reasoning: str  # AI ë¶„ì„ ê·¼ê±°
    recommendations: List[str]  # AI ê¶Œì¥ì‚¬í•­
    false_positive_probability: float  # ì˜¤íƒì§€ í™•ë¥ 
    trend_analysis: str  # ì¶”ì„¸ ë¶„ì„
    severity_score: float  # AI ì‹¬ê°ë„ ì ìˆ˜ (0-100)

# ==================== ì „ì—­ ìƒíƒœ ====================

current_status = {
    "risk_score": 0.0,
    "risk_level": "safe",
    "total_detections": 0,
    "last_detection": None,
    "alerts_today": 0,
    "pipe_status": "ì •ìƒ - ì›í™œí•œ íë¦„",
    "accumulation_rate": 0.0,
    "blockage_percentage": 0.0,
    "garbage_volume": 0.0,
    "flow_restriction": "ì—†ìŒ",
    "accumulated_areas": 0
}

recent_detections: deque = deque(maxlen=100)
recent_alerts: deque = deque(maxlen=50)
connected_clients: List[WebSocket] = []

# 2ì´ˆ ê°„ê²© ì²˜ë¦¬ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ìƒíƒœ ê´€ë¦¬
pending_detections: Dict[str, Dict] = {}  # ì„ì‹œ ì €ì¥ìš©
confirmed_detections: deque = deque(maxlen=100)  # 2ì´ˆ í™•ì •ëœ ê°ì§€ë“¤
CONFIRMATION_TIME_SECONDS = 2  # 2ì´ˆ í™•ì • ì‹œê°„

# ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
current_frame = None
camera_active = False

# ìƒˆë¡œìš´ ìœ„í—˜ë„ ì„ê³„ê°’ (ë‹¤ì¸µì  í‰ê°€ ê¸°ì¤€)
RISK_THRESHOLDS = {
    "safe": (0, 25),      # 0-25ì : ì•ˆì „
    "warning": (26, 50),  # 26-50ì : ì£¼ì˜  
    "caution": (51, 75),  # 51-75ì : ê²½ê³ 
    "danger": (76, 100)   # 76-100ì : ìœ„í—˜
}

# ì“°ë ˆê¸° ìœ í˜•ë³„ ìœ„í—˜ë„ ê°€ì¤‘ì¹˜ (ë§‰í˜ ìœ„í—˜ì„± ê¸°ì¤€)
GARBAGE_RISK_WEIGHTS = {
    # ë†’ì€ ë§‰í˜ ìœ„í—˜ (ìœ ì—°í•œ ì¬ì§ˆ, í° ë¶€í”¼)
    'plastic_bag': 1.8,
    'garbage_bag': 1.8,
    'plastic_film': 1.7,
    'cloth': 1.6,
    'tissue': 1.5,
    'paper_bag': 1.4,
    'food_waste': 1.4,
    
    # ì¤‘ê°„ ë§‰í˜ ìœ„í—˜
    'paper': 1.2,
    'cardboard': 1.1,
    'plastic_container': 1.0,
    'foam': 1.0,
    
    # ë‚®ì€ ë§‰í˜ ìœ„í—˜ (ë‹¨ë‹¨í•œ ì¬ì§ˆ)
    'plastic_bottle': 0.8,
    'glass_bottle': 0.7,
    'metal_can': 0.6,
    'glass': 0.5,
    
    # ê¸°íƒ€
    'other': 1.0
}

# AI ë¶„ì„ ê°€ì¤‘ì¹˜ (ì‹ ë¢°ë„ ê¸°ë°˜)
AI_WEIGHTS = {
    "low": 0.3,
    "medium": 0.6,
    "high": 0.9,
    "critical": 1.2
}

# ê°ì§€ ì‹ ë¢°ë„ ì„ê³„ê°’ (ì ë‹¹íˆ ì¡°ì •)
MIN_CONFIDENCE_THRESHOLD = 0.4  # 40% ì´ìƒ ì‹ ë¢°ë„ë§Œ ì²˜ë¦¬ (ë” ê´€ëŒ€)
MIN_AREA_THRESHOLD = 1000       # ìµœì†Œ ë©´ì  ê°ì†Œ (ë” ì‘ì€ ê°ì²´ë„ ê°ì§€)
MIN_DETECTIONS_FOR_WARNING = 2  # ê²½ê³  ë°œìƒì„ ìœ„í•œ ìµœì†Œ ê°ì§€ íšŸìˆ˜ ê°ì†Œ
MIN_DETECTIONS_FOR_DANGER = 4   # ìœ„í—˜ ë°œìƒì„ ìœ„í•œ ìµœì†Œ ê°ì§€ íšŸìˆ˜ ê°ì†Œ

# ==================== ë¶„ì„ í•¨ìˆ˜ë“¤ ====================

def get_dynamic_thresholds(weather_risk: float = 1.0, seasonal_factor: float = 1.0, location_factor: float = 1.0) -> Dict[str, tuple]:
    """í™˜ê²½ ì¡°ê±´ì— ë”°ë¥¸ ë™ì  ì„ê³„ê°’ ì¡°ì •"""
    base_thresholds = {
        "safe": (0, 25),
        "warning": (26, 50), 
        "caution": (51, 75),
        "danger": (76, 100)
    }
    
    # í™˜ê²½ ìš”ì¸ì„ ì¢…í•©í•œ ì¡°ì • ê³„ìˆ˜
    adjustment_factor = weather_risk * seasonal_factor * location_factor
    
    # ì„ê³„ê°’ ì¡°ì • (ìœ„í—˜ ìƒí™©ì¼ìˆ˜ë¡ ë” ë‚®ì€ ì„ê³„ê°’ ì ìš©)
    adjusted_thresholds = {}
    for level, (low, high) in base_thresholds.items():
        if adjustment_factor > 1.2:  # ë†’ì€ ìœ„í—˜ í™˜ê²½
            adjusted_low = max(0, int(low * 0.8))  # 20% ë‚®ì¶¤
            adjusted_high = max(adjusted_low + 1, int(high * 0.8))
        elif adjustment_factor > 1.0:  # ë³´í†µ ìœ„í—˜ í™˜ê²½
            adjusted_low = max(0, int(low * 0.9))  # 10% ë‚®ì¶¤
            adjusted_high = max(adjusted_low + 1, int(high * 0.9))
        else:  # ë‚®ì€ ìœ„í—˜ í™˜ê²½
            adjusted_low = low
            adjusted_high = high
            
        adjusted_thresholds[level] = (adjusted_low, adjusted_high)
    
    return adjusted_thresholds

def get_garbage_type_risk_weight(garbage_type: str) -> float:
    """ì“°ë ˆê¸° ìœ í˜•ì— ë”°ë¥¸ ìœ„í—˜ë„ ê°€ì¤‘ì¹˜ ë°˜í™˜"""
    # ì“°ë ˆê¸° ìœ í˜• ì •ê·œí™” (ë‹¤ì–‘í•œ í˜•íƒœì˜ ì´ë¦„ ë§¤í•‘)
    normalized_type = garbage_type.lower().replace(' ', '_')
    
    # ìœ í˜•ë³„ ë§¤í•‘
    type_mappings = {
        'plastic': ['plastic_bag', 'plastic_film', 'plastic_container', 'plastic_bottle'],
        'paper': ['paper', 'paper_bag', 'cardboard', 'tissue'],
        'food': ['food_waste', 'organic'],
        'metal': ['metal_can', 'aluminium'],
        'glass': ['glass', 'glass_bottle'],
        'other': ['cloth', 'garbage_bag', 'foam']
    }
    
    # ì§ì ‘ ë§¤ì¹­ ì‹œë„
    if normalized_type in GARBAGE_RISK_WEIGHTS:
        return GARBAGE_RISK_WEIGHTS[normalized_type]
    
    # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ë§¤ì¹­
    for category, types in type_mappings.items():
        if any(t in normalized_type for t in types):
            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í‰ê·  ê°€ì¤‘ì¹˜ ê³„ì‚°
            category_weights = [GARBAGE_RISK_WEIGHTS.get(t, 1.0) for t in types if t in GARBAGE_RISK_WEIGHTS]
            if category_weights:
                return sum(category_weights) / len(category_weights)
    
    # ê¸°ë³¸ê°’
    return GARBAGE_RISK_WEIGHTS.get('other', 1.0)

def analyze_spatiotemporal_patterns(detections: List[DetectionData]) -> Dict[str, float]:
    """ì‹œê³µê°„ì  íŒ¨í„´ ë¶„ì„"""
    if not detections:
        return {
            'accumulation_rate': 0.0,
            'concentration_factor': 0.0,
            'persistence_score': 0.0,
            'spatial_clustering': 0.0,
            'temporal_intensity': 0.0
        }
    
    now = datetime.now()
    
    # 1. ì¶•ì  ì†ë„ ê³„ì‚° (ìµœê·¼ 1ì‹œê°„)
    recent_hour_detections = []
    for d in detections:
        try:
            det_time = datetime.fromisoformat(d.timestamp.replace('Z', '+00:00'))
            hours_ago = (now - det_time).total_seconds() / 3600
            if hours_ago <= 1.0:
                recent_hour_detections.append(d)
        except:
            continue
    
    accumulation_rate = len(recent_hour_detections) / max(1, len(detections)) * 100
    
    # 2. ìœ„ì¹˜ë³„ ì§‘ì¤‘ë„ ë¶„ì„
    location_clusters = {}
    for d in recent_hour_detections:
        bbox = d.bbox
        # 100x100 í”½ì…€ ë‹¨ìœ„ë¡œ ê·¸ë¦¬ë“œ ìƒì„±
        grid_x = bbox[0] // 100
        grid_y = bbox[1] // 100
        grid_key = f"{grid_x}_{grid_y}"
        
        if grid_key not in location_clusters:
            location_clusters[grid_key] = {
                'count': 0,
                'total_area': 0,
                'types': set()
            }
        
        location_clusters[grid_key]['count'] += 1
        location_clusters[grid_key]['total_area'] += d.area
        location_clusters[grid_key]['types'].add(d.garbage_type)
    
    # ì§‘ì¤‘ë„ ì ìˆ˜ ê³„ì‚° (í´ëŸ¬ìŠ¤í„°ë‹¹ í‰ê·  ê°ì§€ ìˆ˜)
    if location_clusters:
        total_detections = sum(cluster['count'] for cluster in location_clusters.values())
        concentration_factor = total_detections / len(location_clusters)
    else:
        concentration_factor = 0.0
    
    # 3. ì‹œê°„ë³„ ì§€ì†ì„± ë¶„ì„ (ê°™ì€ ìœ„ì¹˜ì—ì„œì˜ ì—°ì† ê°ì§€)
    persistence_scores = []
    for grid_key, cluster in location_clusters.items():
        if cluster['count'] >= 3:  # 3íšŒ ì´ìƒ ê°ì§€ëœ ìœ„ì¹˜
            # í•´ë‹¹ ìœ„ì¹˜ì˜ ê°ì§€ ì‹œê°„ ë¶„í¬ ê³„ì‚°
            grid_detections = []
            for d in recent_hour_detections:
                bbox = d.bbox
                if f"{bbox[0]//100}_{bbox[1]//100}" == grid_key:
                    try:
                        det_time = datetime.fromisoformat(d.timestamp.replace('Z', '+00:00'))
                        grid_detections.append(det_time)
                    except:
                        continue
            
            if len(grid_detections) >= 2:
                grid_detections.sort()
                time_span = (grid_detections[-1] - grid_detections[0]).total_seconds() / 3600
                persistence_score = min(time_span * cluster['count'], 10.0)  # ìµœëŒ€ 10ì 
                persistence_scores.append(persistence_score)
    
    avg_persistence = sum(persistence_scores) / len(persistence_scores) if persistence_scores else 0.0
    
    # 4. ê³µê°„ í´ëŸ¬ìŠ¤í„°ë§ ì ìˆ˜ (ì¸ì ‘í•œ ê·¸ë¦¬ë“œì˜ ê°ì§€ ë°€ë„)
    spatial_clustering = 0.0
    for grid_key in location_clusters:
        x, y = map(int, grid_key.split('_'))
        # ì£¼ë³€ 8ê°œ ì…€ í™•ì¸
        neighbor_count = 0
        for dx in [-1, 0, 1]:
            for dy in [-1, 0, 1]:
                if dx == 0 and dy == 0:
                    continue
                neighbor_key = f"{x+dx}_{y+dy}"
                if neighbor_key in location_clusters:
                    neighbor_count += 1
        
        # ì£¼ë³€ í´ëŸ¬ìŠ¤í„°ê°€ ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        spatial_clustering += neighbor_count * location_clusters[grid_key]['count']
    
    spatial_clustering = min(spatial_clustering / len(location_clusters) if location_clusters else 0, 20.0)
    
    # 5. ì‹œê°„ì  ì§‘ì¤‘ë„ (ë‹¨ìœ„ ì‹œê°„ë‹¹ ê°ì§€ ë¹ˆë„ ë³€í™”)
    time_intervals = []
    if len(recent_hour_detections) >= 2:
        times = []
        for d in recent_hour_detections:
            try:
                det_time = datetime.fromisoformat(d.timestamp.replace('Z', '+00:00'))
                times.append(det_time)
            except:
                continue
        
        times.sort()
        for i in range(1, len(times)):
            interval = (times[i] - times[i-1]).total_seconds() / 60  # ë¶„ ë‹¨ìœ„
            time_intervals.append(interval)
        
        if time_intervals:
            avg_interval = sum(time_intervals) / len(time_intervals)
            # ê°„ê²©ì´ ì§§ì„ìˆ˜ë¡ ë†’ì€ ì§‘ì¤‘ë„
            temporal_intensity = max(0, 10 - avg_interval) if avg_interval < 10 else 0
        else:
            temporal_intensity = 0.0
    else:
        temporal_intensity = 0.0
    
    return {
        'accumulation_rate': round(accumulation_rate, 2),
        'concentration_factor': round(concentration_factor, 2),
        'persistence_score': round(avg_persistence, 2),
        'spatial_clustering': round(spatial_clustering, 2),
        'temporal_intensity': round(temporal_intensity, 2)
    }

def calculate_environmental_risk_factors() -> Dict[str, float]:
    """í™˜ê²½ì  ìœ„í—˜ ìš”ì¸ ê³„ì‚° (ì‹¤ì œ êµ¬í˜„ì‹œ ì™¸ë¶€ API ì—°ë™)"""
    # ì‹¤ì œ êµ¬í˜„ì‹œì—ëŠ” ê¸°ìƒì²­ API, ê³„ì ˆ ì •ë³´ ë“±ì„ í™œìš©
    current_month = datetime.now().month
    
    # ê³„ì ˆë³„ ìœ„í—˜ ìš”ì¸
    seasonal_risk = 1.0
    if current_month in [6, 7, 8]:  # ì—¬ë¦„ (ì¥ë§ˆì² )
        seasonal_risk = 1.3
    elif current_month in [9, 10, 11]:  # ê°€ì„ (ë‚™ì—½ì² )
        seasonal_risk = 1.2
    elif current_month in [12, 1, 2]:  # ê²¨ìš¸ (ë™ê²°)
        seasonal_risk = 1.1
    
    # ì‹œê°„ëŒ€ë³„ ìœ„í—˜ ìš”ì¸ (ì¶œí‡´ê·¼ ì‹œê°„ëŒ€ ì“°ë ˆê¸° ì¦ê°€)
    current_hour = datetime.now().hour
    time_risk = 1.0
    if current_hour in [7, 8, 9, 17, 18, 19]:  # ì¶œí‡´ê·¼ ì‹œê°„
        time_risk = 1.1
    
    return {
        'weather_risk': 1.0,  # ì‹¤ì œë¡œëŠ” ê¸°ìƒ APIì—ì„œ ê°€ì ¸ì˜´
        'seasonal_factor': seasonal_risk,
        'time_factor': time_risk,
        'location_factor': 1.0  # ì‹¤ì œë¡œëŠ” ì§€ì—­ë³„ íŠ¹ì„± ë°˜ì˜
    }

def generate_detection_key(detection: DetectionData) -> str:
    """ê°ì§€ ì‹ë³„í‚¤ ìƒì„± (ìœ„ì¹˜ì™€ ìœ í˜• ê¸°ë°˜)"""
    bbox = detection.bbox
    center_x = (bbox[0] + bbox[2]) // 2
    center_y = (bbox[1] + bbox[3]) // 2
    # 100í”½ì…€ ê·¸ë¦¬ë“œë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì‘ì€ ì›€ì§ì„ ë¬´ì‹œ
    grid_x = center_x // 100
    grid_y = center_y // 100
    return f"{detection.garbage_type}_{grid_x}_{grid_y}"

def check_and_confirm_detections():
    """ëŒ€ê¸° ì¤‘ì¸ ê°ì§€ë“¤ì„ í™•ì¸í•˜ê³  2ì´ˆ ì§€ì†ëœ ê²ƒë“¤ì„ í™•ì •"""
    global pending_detections
    now = datetime.now()
    
    confirmed_keys = []
    for key, detection_info in pending_detections.items():
        first_detected = detection_info['first_detected']
        time_diff = (now - first_detected).total_seconds()
        
        if time_diff >= CONFIRMATION_TIME_SECONDS:
            # 2ì´ˆ ì§€ì†ëœ ê°ì§€ë¥¼ í™•ì •
            confirmed_detection = detection_info['detection']
            confirmed_detections.append(confirmed_detection)
            confirmed_keys.append(key)
            
            logger.info(f"âœ… 2ì´ˆ ì§€ì† í™•ì •: {confirmed_detection.garbage_type} at {key}")
            return confirmed_detection
    
    # í™•ì •ëœ ê°ì§€ë“¤ì„ ëŒ€ê¸° ëª©ë¡ì—ì„œ ì œê±°
    for key in confirmed_keys:
        del pending_detections[key]
    
    return None

def add_to_pending_detections(detection: DetectionData):
    """ìƒˆë¡œìš´ ê°ì§€ë¥¼ ëŒ€ê¸° ëª©ë¡ì— ì¶”ê°€"""
    global pending_detections
    key = generate_detection_key(detection)
    now = datetime.now()
    
    if key not in pending_detections:
        # ìƒˆë¡œìš´ ê°ì§€
        pending_detections[key] = {
            'detection': detection,
            'first_detected': now,
            'last_updated': now,
            'count': 1
        }
        logger.debug(f"â³ ìƒˆ ê°ì§€ ëŒ€ê¸°: {detection.garbage_type} at {key}")
    else:
        # ê¸°ì¡´ ê°ì§€ ì—…ë°ì´íŠ¸
        pending_detections[key]['last_updated'] = now
        pending_detections[key]['count'] += 1
        pending_detections[key]['detection'] = detection  # ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
        logger.debug(f"ğŸ”„ ê°ì§€ ì—…ë°ì´íŠ¸: {detection.garbage_type} at {key} (count: {pending_detections[key]['count']})")

def cleanup_old_pending_detections():
    """ì˜¤ë˜ëœ ëŒ€ê¸° ê°ì§€ë“¤ì„ ì •ë¦¬ (5ì´ˆ ì´ìƒ ì—…ë°ì´íŠ¸ ì—†ìŒ)"""
    global pending_detections
    now = datetime.now()
    
    old_keys = []
    for key, detection_info in pending_detections.items():
        time_since_update = (now - detection_info['last_updated']).total_seconds()
        if time_since_update > 5.0:  # 5ì´ˆ ì´ìƒ ì—…ë°ì´íŠ¸ ì—†ìŒ
            old_keys.append(key)
    
    for key in old_keys:
        logger.debug(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ëŒ€ê¸° ê°ì§€ ì œê±°: {key}")
        del pending_detections[key]

def is_duplicate_detection(new_detection: DetectionData, threshold_seconds: int = 10) -> bool:
    """ì¤‘ë³µ ê°ì§€ì¸ì§€ í™•ì¸"""
    if not recent_detections:
        return False
    
    try:
        new_time = datetime.fromisoformat(new_detection.timestamp.replace('Z', '+00:00'))
        new_bbox = new_detection.bbox
        new_center = ((new_bbox[0] + new_bbox[2]) // 2, (new_bbox[1] + new_bbox[3]) // 2)
        
        for detection in list(recent_detections)[-5:]:
            try:
                det_time = datetime.fromisoformat(detection.timestamp.replace('Z', '+00:00'))
                time_diff = (new_time - det_time).total_seconds()
                
                if time_diff < threshold_seconds:
                    det_bbox = detection.bbox
                    det_center = ((det_bbox[0] + det_bbox[2]) // 2, (det_bbox[1] + det_bbox[3]) // 2)
                    distance = math.sqrt((new_center[0] - det_center[0])**2 + (new_center[1] - det_center[1])**2)
                    
                    if distance < 50 and detection.garbage_type == new_detection.garbage_type:
                        return True
            except:
                continue
                
    except:
        pass
    
    return False

def analyze_pipe_blockage(detections: List[DetectionData]) -> BlockageAnalysis:
    """í•˜ìˆ˜êµ¬ ë§‰í˜ ì •ë„ ë¶„ì„"""
    if not detections:
        return BlockageAnalysis(
            blockage_percentage=0.0,
            garbage_volume=0.0,
            flow_restriction="ì—†ìŒ",
            accumulated_areas=0,
            total_area=0
        )
    
    now = datetime.now()
    recent_hour = now - timedelta(hours=1)
    
    # ìµœê·¼ 1ì‹œê°„ ë‚´ ê°ì§€ í•„í„°ë§
    recent_detections_list = []
    for d in detections:
        try:
            detection_time = datetime.fromisoformat(d.timestamp.replace('Z', '+00:00'))
            if detection_time >= recent_hour:
                recent_detections_list.append(d)
        except:
            recent_detections_list.append(d)
    
    # ì¶•ì  ì˜ì—­ ë¶„ì„
    blockage_areas = {}
    total_area = 0
    
    for detection in recent_detections_list:
        bbox = detection.bbox
        area_key = f"{bbox[0]//100}_{bbox[1]//100}"
        
        if area_key not in blockage_areas:
            blockage_areas[area_key] = {
                "count": 0,
                "total_area": 0,
                "max_confidence": 0,
                "garbage_types": set()
            }
        
        blockage_areas[area_key]["count"] += 1
        blockage_areas[area_key]["total_area"] += detection.area
        blockage_areas[area_key]["max_confidence"] = max(
            blockage_areas[area_key]["max_confidence"], 
            detection.confidence
        )
        blockage_areas[area_key]["garbage_types"].add(detection.garbage_type)
        total_area += detection.area
    
    # ë§‰í˜ ì •ë„ ê³„ì‚°
    pipe_width = 640
    pipe_height = 480
    total_pipe_area = pipe_width * pipe_height
    
    blockage_percentage = min((total_area / total_pipe_area) * 100, 100)
    
    # íë¦„ ì œí•œ ìˆ˜ì¤€ ê²°ì •
    if blockage_percentage < 10:
        flow_restriction = "ë¯¸ë¯¸í•¨"
    elif blockage_percentage < 30:
        flow_restriction = "ê²½ë¯¸í•¨"
    elif blockage_percentage < 60:
        flow_restriction = "ë³´í†µ"
    elif blockage_percentage < 80:
        flow_restriction = "ì‹¬ê°í•¨"
    else:
        flow_restriction = "ë§¤ìš° ì‹¬ê°í•¨"
    
    # ìš©ì  ì¶”ì •
    estimated_depth = 5  # cm
    garbage_volume = (total_area / 10000) * estimated_depth  # cmÂ³
    
    return BlockageAnalysis(
        blockage_percentage=round(blockage_percentage, 1),
        garbage_volume=round(garbage_volume, 2),
        flow_restriction=flow_restriction,
        accumulated_areas=len(blockage_areas),
        total_area=total_area
    )

def is_valid_detection(detection: DetectionData) -> bool:
    """ê°ì§€ê°€ ìœ íš¨í•œì§€ ê²€ì‚¬"""
    # ì‹ ë¢°ë„ ì²´í¬
    if detection.confidence < MIN_CONFIDENCE_THRESHOLD:
        return False
    
    # ìµœì†Œ ë©´ì  ì²´í¬
    if detection.area < MIN_AREA_THRESHOLD:
        return False
    
    # ë°”ìš´ë”© ë°•ìŠ¤ ìœ íš¨ì„± ì²´í¬
    bbox = detection.bbox
    if len(bbox) != 4 or bbox[2] <= bbox[0] or bbox[3] <= bbox[1]:
        return False
    
    return True

def calculate_risk_score_with_ai(detections: List[DetectionData]) -> tuple[float, AIAnalysis]:
    """ë‹¤ì¸µì  ìœ„í—˜ë„ í‰ê°€ ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°œì„ ëœ ìœ„í—˜ë„ ê³„ì‚°"""
    current_risk = current_status.get("risk_score", 0)
    
    # í™˜ê²½ì  ìœ„í—˜ ìš”ì¸ ê³„ì‚°
    env_factors = calculate_environmental_risk_factors()
    
    # ë™ì  ì„ê³„ê°’ ì¡°ì •
    dynamic_thresholds = get_dynamic_thresholds(
        weather_risk=env_factors['weather_risk'],
        seasonal_factor=env_factors['seasonal_factor'],
        location_factor=env_factors['location_factor']
    )
    
    if not detections:
        # ê°ì§€ê°€ ì—†ìœ¼ë©´ ìœ„í—˜ë„ë¥¼ 0ìœ¼ë¡œ ì„¤ì • (ì¦‰ì‹œ ë°˜ì˜)
        base_score = 0.0
        
        ai_analysis = AIAnalysis(
            risk_assessment="low",
            confidence_level=0.9,
            reasoning="ê°ì§€ëœ ì“°ë ˆê¸°ê°€ ì—†ì–´ ì•ˆì „í•œ ìƒíƒœì…ë‹ˆë‹¤.",
            recommendations=["ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê³„ì†í•˜ì„¸ìš”."],
            false_positive_probability=0.0,
            trend_analysis="ê°œì„ ",
            severity_score=0.0
        )
        return base_score, ai_analysis
    
    # ìœ íš¨í•œ ê°ì§€ë§Œ í•„í„°ë§
    valid_detections = [d for d in detections if is_valid_detection(d)]
    
    if not valid_detections:
        base_score = max(0, current_status.get("risk_score", 0) - 5)  # ë” ë¹ ë¥¸ ê°ì†Œ
        ai_analysis = AIAnalysis(
            risk_assessment="low",
            confidence_level=0.6,
            reasoning="ìœ íš¨í•œ ê°ì§€ê°€ ì—†ì–´ ì•ˆì „í•œ ìƒíƒœì…ë‹ˆë‹¤.",
            recommendations=["ì¹´ë©”ë¼ ì‹œìŠ¤í…œì„ ì ê²€í•˜ì„¸ìš”.", "ê°ì§€ ì •í™•ë„ë¥¼ ë†’ì´ì„¸ìš”."],
            false_positive_probability=0.7,
            trend_analysis="ë¶ˆí™•ì‹¤",
            severity_score=0.0
        )
        return base_score, ai_analysis
    
    # === 1. ë¬¼ë¦¬ì  ë§‰í˜ë„ ê³„ì‚° (40% ê°€ì¤‘ì¹˜) ===
    blockage_analysis = analyze_pipe_blockage(valid_detections)
    
    # ì“°ë ˆê¸° ìœ í˜•ë³„ ìœ„í—˜ë„ ê°€ì¤‘ì¹˜ ì ìš©
    type_weighted_score = 0.0
    total_area_weighted = 0.0
    
    for detection in valid_detections:
        type_weight = get_garbage_type_risk_weight(detection.garbage_type)
        weighted_area = detection.area * type_weight
        total_area_weighted += weighted_area
        type_weighted_score += detection.confidence * type_weight * (detection.area / 10000)
    
    # ë¬¼ë¦¬ì  ë§‰í˜ë„ ì ìˆ˜ (ì“°ë ˆê¸° ìˆ˜ ë°˜ì˜)
    detection_count_bonus = min(len(valid_detections) * 2, 15)  # ê°ì§€ ê°œìˆ˜ ë³´ë„ˆìŠ¤
    physical_blockage_score = min(
        (blockage_analysis.blockage_percentage * 0.5) +
        (type_weighted_score * 0.3) +
        (blockage_analysis.accumulated_areas * 0.1) +
        detection_count_bonus,
        40.0  # ìµœëŒ€ 40ì 
    )
    
    # === 2. í™˜ê²½ì  ìš”ì¸ ê³„ì‚° (30% ê°€ì¤‘ì¹˜) ===
    seasonal_bonus = (env_factors['seasonal_factor'] - 1.0) * 10  # ê³„ì ˆë³„ ì¶”ê°€ ì ìˆ˜
    time_bonus = (env_factors['time_factor'] - 1.0) * 5  # ì‹œê°„ëŒ€ë³„ ì¶”ê°€ ì ìˆ˜
    
    environmental_score = min(seasonal_bonus + time_bonus, 30.0)  # ìµœëŒ€ 30ì 
    
    # === 3. ì‹œê°„ì  íŒ¨í„´ ê³„ì‚° (20% ê°€ì¤‘ì¹˜) ===
    spatiotemporal_patterns = analyze_spatiotemporal_patterns(valid_detections)
    
    pattern_score = min(
        (spatiotemporal_patterns['accumulation_rate'] * 0.3) +
        (spatiotemporal_patterns['concentration_factor'] * 0.3) +
        (spatiotemporal_patterns['persistence_score'] * 0.2) +
        (spatiotemporal_patterns['spatial_clustering'] * 0.1) +
        (spatiotemporal_patterns['temporal_intensity'] * 0.1),
        20.0  # ìµœëŒ€ 20ì 
    )
    
    # === 4. AI ì‹ ë¢°ë„ ë³´ì • (10% ê°€ì¤‘ì¹˜) ===
    ai_analysis = analyze_with_ai(valid_detections, blockage_analysis)
    
    # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
    avg_confidence = sum(d.confidence for d in valid_detections) / len(valid_detections)
    confidence_score = min((avg_confidence - 0.7) * 20, 10.0) if avg_confidence > 0.7 else 0
    
    # AI ì‹ ë¢°ë„ ë³´ì • ì ìˆ˜
    ai_reliability_score = min(
        confidence_score * (1.0 - ai_analysis.false_positive_probability),
        10.0  # ìµœëŒ€ 10ì 
    )
    
    # === ìµœì¢… ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° ===
    base_risk_score = (
        physical_blockage_score +      # 40%
        environmental_score +          # 30%
        pattern_score +               # 20%
        ai_reliability_score          # 10%
    )
    
    # AI ë¶„ì„ ê°€ì¤‘ì¹˜ ì ìš©
    ai_weight = AI_WEIGHTS.get(ai_analysis.risk_assessment, 1.0)
    adjusted_score = base_risk_score * ai_weight
    
    # ë™ì  ë³€í™” ì ìš©
    dynamic_change = calculate_enhanced_risk_change(valid_detections, current_risk, spatiotemporal_patterns)
    
    if dynamic_change < 0:  # ê°ì†Œí•˜ëŠ” ê²½ìš°
        final_score = max(0, current_risk + dynamic_change * 1.5)
    else:  # ì¦ê°€í•˜ëŠ” ê²½ìš°
        final_score = min(100, current_risk + dynamic_change * 0.8)
    
    # í˜„ì¬ ìœ„í—˜ë„ì™€ ìƒˆë¡œ ê³„ì‚°ëœ ìœ„í—˜ë„ ë¹„êµí•˜ì—¬ ë” ë‚®ì€ ê°’ ì‚¬ìš©
    combined_score = min(adjusted_score, current_risk) if adjusted_score < current_risk else adjusted_score
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    current_status.update({
        "blockage_percentage": blockage_analysis.blockage_percentage,
        "garbage_volume": blockage_analysis.garbage_volume,
        "flow_restriction": blockage_analysis.flow_restriction,
        "accumulated_areas": blockage_analysis.accumulated_areas,
        "ai_analysis": ai_analysis.model_dump(),
        "environmental_factors": env_factors,
        "spatiotemporal_patterns": spatiotemporal_patterns,
        "physical_score": physical_blockage_score,
        "environmental_score": environmental_score,
        "pattern_score": pattern_score,
        "ai_score": ai_reliability_score
    })
    
    return min(combined_score, 100.0), ai_analysis

def calculate_enhanced_risk_change(detections: List[DetectionData], current_risk: float, patterns: Dict[str, float]) -> float:
    """ê°œì„ ëœ ë™ì  ìœ„í—˜ë„ ë³€í™” ê³„ì‚°"""
    if not detections:
        time_since_last = get_time_since_last_detection()
        decay_rate = 2.0  # ë¶„ë‹¹ 2% ê°ì†Œ
        return -min(decay_rate * time_since_last, current_risk)
    
    # ì¶•ì  ì†ë„ ê¸°ë°˜ ë³€í™”
    accumulation_change = patterns['accumulation_rate'] * 0.3
    
    # ê³µê°„ì  ì§‘ì¤‘ë„ ê¸°ë°˜ ë³€í™”
    concentration_change = patterns['concentration_factor'] * 0.2
    
    # ì‹œê°„ì  ì§‘ì¤‘ë„ ê¸°ë°˜ ë³€í™”
    temporal_change = patterns['temporal_intensity'] * 0.3
    
    # ì§€ì†ì„± ê¸°ë°˜ ë³€í™”
    persistence_change = patterns['persistence_score'] * 0.2
    
    total_change = accumulation_change + concentration_change + temporal_change + persistence_change
    
    # ìµœëŒ€ ë³€í™”ëŸ‰ ì œí•œ
    return min(max(total_change, -10), 15)

def calculate_risk_score(detections: List[DetectionData]) -> float:
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    score, _ = calculate_risk_score_with_ai(detections)
    return score

def get_risk_level(score: float) -> str:
    """ìƒˆë¡œìš´ 4ë‹¨ê³„ ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •"""
    if score >= RISK_THRESHOLDS["danger"][0]:
        return "danger"
    elif score >= RISK_THRESHOLDS["caution"][0]:
        return "caution"
    elif score >= RISK_THRESHOLDS["warning"][0]:
        return "warning"
    else:
        return "safe"

def get_pipe_status(risk_level: str) -> str:
    """ìƒˆë¡œìš´ 4ë‹¨ê³„ íŒŒì´í”„ ìƒíƒœ í…ìŠ¤íŠ¸"""
    status_map = {
        "safe": "ì •ìƒ - ì›í™œí•œ íë¦„",
        "warning": "ì£¼ì˜ - ì¶•ì ëŸ‰ ì¦ê°€", 
        "caution": "ê²½ê³  - ë§‰í˜ ìœ„í—˜ ì¦ê°€",
        "danger": "ìœ„í—˜ - ë§‰í˜ ê°€ëŠ¥ì„± ë†’ìŒ"
    }
    return status_map.get(risk_level, "ì•Œ ìˆ˜ ì—†ìŒ")

def get_time_since_last_detection() -> float:
    """ë§ˆì§€ë§‰ ê°ì§€ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„ (ë¶„)"""
    if not recent_detections:
        return 10.0  # ê¸°ë³¸ê°’: 10ë¶„
    
    try:
        last_detection = recent_detections[-1]
        last_time = datetime.fromisoformat(last_detection.timestamp.replace('Z', '+00:00'))
        now = datetime.now()
        time_diff = (now - last_time).total_seconds() / 60  # ë¶„ ë‹¨ìœ„
        return max(0.1, time_diff)  # ìµœì†Œ 0.1ë¶„
    except:
        return 5.0  # ì˜¤ë¥˜ì‹œ ê¸°ë³¸ê°’

def calculate_dynamic_risk_change(detections: List[DetectionData], current_risk: float) -> float:
    """ë™ì  ìœ„í—˜ë„ ë³€í™” ê³„ì‚°"""
    if not detections:
        # ì“°ë ˆê¸°ê°€ ì—†ìœ¼ë©´ ìì—° ê°ì†Œ
        time_since_last = get_time_since_last_detection()
        decay_rate = 1.5  # ë¶„ë‹¹ 1.5% ê°ì†Œ
        decay_amount = min(decay_rate * time_since_last, current_risk)
        return -decay_amount
    
    # ìµœê·¼ ê°ì§€ ë¶„ì„ (ìµœê·¼ 5ë¶„)
    now = datetime.now()
    recent_detections_5min = []
    
    for d in detections[-10:]:  # ìµœê·¼ 10ê°œë§Œ í™•ì¸
        try:
            det_time = datetime.fromisoformat(d.timestamp.replace('Z', '+00:00'))
            minutes_ago = (now - det_time).total_seconds() / 60
            if minutes_ago <= 5:  # 5ë¶„ ì´ë‚´
                recent_detections_5min.append(d)
        except:
            continue
    
    if not recent_detections_5min:
        # ìµœê·¼ 5ë¶„ ë‚´ ê°ì§€ê°€ ì—†ìœ¼ë©´ ê°ì†Œ
        time_since_last = get_time_since_last_detection()
        decay_rate = 1.0  # ë¶„ë‹¹ 1% ê°ì†Œ
        decay_amount = min(decay_rate * time_since_last, current_risk)
        return -decay_amount
    
    # ìµœê·¼ ê°ì§€ê°€ ìˆìœ¼ë©´ ìœ„í—˜ë„ ì¦ê°€ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
    recent_count = len(recent_detections_5min)
    avg_confidence = sum(d.confidence for d in recent_detections_5min) / len(recent_detections_5min)
    
    # ì‹ ë¢°ë„ê°€ ë†’ê³  ê°ì§€ê°€ ë§ì„ìˆ˜ë¡ ìœ„í—˜ë„ ì¦ê°€ (ë” ì œí•œì ìœ¼ë¡œ)
    increase_rate = min(recent_count * 2 * avg_confidence, 8)  # ìµœëŒ€ 8% ì¦ê°€
    
    return increase_rate

def analyze_with_ai(detections: List[DetectionData], blockage_analysis: BlockageAnalysis) -> AIAnalysis:
    """AI ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ì„"""
    if not detections:
        return AIAnalysis(
            risk_assessment="low",
            confidence_level=0.9,
            reasoning="ê°ì§€ëœ ì“°ë ˆê¸°ê°€ ì—†ì–´ ì•ˆì „í•œ ìƒíƒœì…ë‹ˆë‹¤.",
            recommendations=["ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê³„ì†í•˜ì„¸ìš”."],
            false_positive_probability=0.0,
            trend_analysis="ì•ˆì •ì ",
            severity_score=0.0
        )
    
    # ìµœê·¼ ê°ì§€ ë¶„ì„ (ìµœê·¼ 10ê°œ)
    recent_detections = detections[-10:] if len(detections) > 10 else detections
    now = datetime.now()
    
    # ì‹œê°„ ë¶„í¬ ë¶„ì„
    time_distribution = []
    for d in recent_detections:
        try:
            det_time = datetime.fromisoformat(d.timestamp.replace('Z', '+00:00'))
            minutes_ago = (now - det_time).total_seconds() / 60
            time_distribution.append(minutes_ago)
        except:
            continue
    
    # ì‹ ë¢°ë„ ë¶„ì„
    avg_confidence = sum(d.confidence for d in recent_detections) / len(recent_detections)
    high_confidence_count = sum(1 for d in recent_detections if d.confidence > 0.8)
    confidence_ratio = high_confidence_count / len(recent_detections)
    
    # ì“°ë ˆê¸° ìœ í˜• ë¶„ì„
    garbage_types = {}
    for d in recent_detections:
        garbage_types[d.garbage_type] = garbage_types.get(d.garbage_type, 0) + 1
    
    # AI ë¶„ì„ ë¡œì§
    risk_factors = []
    severity_score = 0.0
    
    # ë™ì  ë³€í™” ë¶„ì„
    current_risk = current_status.get("risk_score", 0)
    previous_risk = current_status.get("previous_risk_score", current_risk)
    risk_change = current_risk - previous_risk
    
    # 1. ë§‰í˜ë¥  ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if blockage_analysis.blockage_percentage > 80:
        risk_factors.append("ë§‰í˜ë¥ ì´ 80%ë¥¼ ì´ˆê³¼í•˜ì—¬ ë§¤ìš° ì‹¬ê°í•œ ìƒí™©")
        severity_score += 40
    elif blockage_analysis.blockage_percentage > 60:
        risk_factors.append("ë§‰í˜ë¥ ì´ 60%ë¥¼ ì´ˆê³¼í•˜ì—¬ ì‹¬ê°í•œ ìƒí™©")
        severity_score += 25
    elif blockage_analysis.blockage_percentage > 40:
        risk_factors.append("ë§‰í˜ë¥ ì´ 40%ë¥¼ ì´ˆê³¼í•˜ì—¬ ì£¼ì˜ê°€ í•„ìš”")
        severity_score += 10
    elif blockage_analysis.blockage_percentage > 20:
        risk_factors.append("ë§‰í˜ë¥ ì´ 20%ë¥¼ ì´ˆê³¼í•˜ì—¬ ëª¨ë‹ˆí„°ë§ í•„ìš”")
        severity_score += 5
    
    # 2. ì‹ ë¢°ë„ ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if avg_confidence < 0.8:
        risk_factors.append("í‰ê·  ì‹ ë¢°ë„ê°€ ë‚®ì•„ ì˜¤íƒì§€ ê°€ëŠ¥ì„± ë†’ìŒ")
        severity_score -= 15  # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ìœ„í—˜ë„ ëŒ€í­ ê°ì†Œ
    elif avg_confidence > 0.95:
        risk_factors.append("ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„ë¡œ ì •í™•í•œ ê°ì§€")
        severity_score += 8
    elif avg_confidence > 0.85:
        risk_factors.append("ë†’ì€ ì‹ ë¢°ë„ë¡œ ì •í™•í•œ ê°ì§€")
        severity_score += 3
    
    # 3. ì‹œê°„ ë¶„í¬ ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if time_distribution:
        recent_count = sum(1 for t in time_distribution if t <= 5)  # 5ë¶„ ì´ë‚´
        if recent_count >= 5:
            risk_factors.append("ìµœê·¼ 5ë¶„ ë‚´ ë‹¤ìˆ˜ ê°ì§€ë¡œ ê¸‰ì†í•œ ì¶•ì ")
            severity_score += 25
        elif recent_count >= 3:
            risk_factors.append("ìµœê·¼ 5ë¶„ ë‚´ ì—¬ëŸ¬ ê°ì§€ë¡œ ì¶•ì  ì¦ê°€")
            severity_score += 15
        elif recent_count >= 1:
            risk_factors.append("ìµœê·¼ ê°ì§€ë¡œ ì§€ì†ì  ëª¨ë‹ˆí„°ë§ í•„ìš”")
            severity_score += 5
    
    # 4. ì“°ë ˆê¸° ìœ í˜• ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    dangerous_types = ["plastic_bag", "cloth", "paper", "organic"]
    dangerous_count = sum(garbage_types.get(t, 0) for t in dangerous_types)
    if dangerous_count >= 5:
        risk_factors.append("ë§‰í˜ ìœ„í—˜ì´ ë†’ì€ ì“°ë ˆê¸° ë‹¤ìˆ˜ ê°ì§€")
        severity_score += 20
    elif dangerous_count >= 3:
        risk_factors.append("ë§‰í˜ ìœ„í—˜ì´ ë†’ì€ ì“°ë ˆê¸° ì—¬ëŸ¬ ê°œ ê°ì§€")
        severity_score += 10
    
    # 5. ì¶•ì  ì˜ì—­ ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if blockage_analysis.accumulated_areas >= 8:
        risk_factors.append("ë‹¤ìˆ˜ì˜ ì¶•ì  ì˜ì—­ìœ¼ë¡œ ë¶„ì‚°ëœ ë§‰í˜")
        severity_score += 15
    elif blockage_analysis.accumulated_areas >= 5:
        risk_factors.append("ì—¬ëŸ¬ ì¶•ì  ì˜ì—­ìœ¼ë¡œ ë§‰í˜ ìœ„í—˜")
        severity_score += 8
    
    # 6. ë™ì  ë³€í™” ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if risk_change > 10:
        risk_factors.append("ìœ„í—˜ë„ê°€ ê¸‰ì†íˆ ì¦ê°€í•˜ëŠ” ìƒí™©")
        severity_score += 20
    elif risk_change > 5:
        risk_factors.append("ìœ„í—˜ë„ê°€ ì ì§„ì ìœ¼ë¡œ ì¦ê°€")
        severity_score += 10
    elif risk_change < -10:
        risk_factors.append("ìœ„í—˜ë„ê°€ ê¸‰ì†íˆ ê°ì†Œí•˜ëŠ” ê°œì„  ìƒí™©")
        severity_score -= 15
    elif risk_change < -5:
        risk_factors.append("ìœ„í—˜ë„ê°€ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ")
        severity_score -= 8
    
    # 7. ì¶”ì„¸ ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if len(detections) >= 30:  # ë” ë§ì€ ë°ì´í„° í•„ìš”
        recent_15 = detections[-15:]
        older_15 = detections[-30:-15]
        recent_avg = sum(d.area for d in recent_15) / len(recent_15)
        older_avg = sum(d.area for d in older_15) / len(older_15)
        
        if recent_avg > older_avg * 2.0:  # ë” í° ì¦ê°€ í•„ìš”
            risk_factors.append("ì“°ë ˆê¸° í¬ê¸°ê°€ ê¸‰ì†íˆ ì¦ê°€í•˜ëŠ” ì¶”ì„¸")
            severity_score += 15
        elif recent_avg > older_avg * 1.5:
            risk_factors.append("ì“°ë ˆê¸° í¬ê¸°ê°€ ì¦ê°€í•˜ëŠ” ì¶”ì„¸")
            severity_score += 8
        elif recent_avg < older_avg * 0.3:  # ë” í° ê°ì†Œ í•„ìš”
            risk_factors.append("ì“°ë ˆê¸° í¬ê¸°ê°€ ê¸‰ì†íˆ ê°ì†Œí•˜ëŠ” ì¶”ì„¸")
            severity_score -= 10
        elif recent_avg < older_avg * 0.5:
            risk_factors.append("ì“°ë ˆê¸° í¬ê¸°ê°€ ê°ì†Œí•˜ëŠ” ì¶”ì„¸")
            severity_score -= 5
    
    # ì˜¤íƒì§€ í™•ë¥  ê³„ì‚° (ë” ì—„ê²©í•˜ê²Œ)
    false_positive_prob = 0.0
    if avg_confidence < 0.8:
        false_positive_prob = 0.4
    if len(recent_detections) < 5:
        false_positive_prob += 0.3
    if len(recent_detections) < 3:
        false_positive_prob += 0.2
    
    # ìœ„í—˜ë„ í‰ê°€ (ë” ì—„ê²©í•˜ê²Œ)
    if severity_score >= 70:  # ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
        risk_assessment = "critical"
        if risk_change > 10:
            reasoning = "ë‹¤ì¤‘ ìœ„í—˜ ìš”ì†Œê°€ ë³µí•©ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ë§¤ìš° ìœ„í—˜í•œ ìƒí™©ì´ë©°, ìœ„í—˜ë„ê°€ ê¸‰ì†íˆ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        else:
            reasoning = "ë‹¤ì¤‘ ìœ„í—˜ ìš”ì†Œê°€ ë³µí•©ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ë§¤ìš° ìœ„í—˜í•œ ìƒí™©"
    elif severity_score >= 50:  # ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
        risk_assessment = "high"
        if risk_change > 5:
            reasoning = "ì—¬ëŸ¬ ìœ„í—˜ ìš”ì†Œê°€ í™•ì¸ë˜ì–´ ë†’ì€ ìœ„í—˜ë„ì´ë©°, ìœ„í—˜ë„ê°€ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ì…ë‹ˆë‹¤."
        else:
            reasoning = "ì—¬ëŸ¬ ìœ„í—˜ ìš”ì†Œê°€ í™•ì¸ë˜ì–´ ë†’ì€ ìœ„í—˜ë„"
    elif severity_score >= 25:  # ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
        risk_assessment = "medium"
        if risk_change < -5:
            reasoning = "ì¼ë¶€ ìœ„í—˜ ìš”ì†Œê°€ í™•ì¸ë˜ì–´ ì£¼ì˜ê°€ í•„ìš”í•˜ì§€ë§Œ, ìœ„í—˜ë„ê°€ ê°ì†Œí•˜ëŠ” ê°œì„  ì¶”ì„¸ì…ë‹ˆë‹¤."
        else:
            reasoning = "ì¼ë¶€ ìœ„í—˜ ìš”ì†Œê°€ í™•ì¸ë˜ì–´ ì£¼ì˜ê°€ í•„ìš”"
    else:
        risk_assessment = "low"
        if risk_change < -10:
            reasoning = "ëŒ€ë¶€ë¶„ì˜ ì§€í‘œê°€ ì•ˆì „ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©°, ìœ„í—˜ë„ê°€ ê¸‰ì†íˆ ê°ì†Œí•˜ëŠ” ì¢‹ì€ ìƒí™©ì…ë‹ˆë‹¤."
        elif risk_change < -5:
            reasoning = "ëŒ€ë¶€ë¶„ì˜ ì§€í‘œê°€ ì•ˆì „ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©°, ìœ„í—˜ë„ê°€ ê°ì†Œí•˜ëŠ” ê°œì„  ì¶”ì„¸ì…ë‹ˆë‹¤."
        else:
            reasoning = "ëŒ€ë¶€ë¶„ì˜ ì§€í‘œê°€ ì•ˆì „ ë²”ìœ„ ë‚´ì— ìˆìŒ"
    
    # ê¶Œì¥ì‚¬í•­ ìƒì„±
    recommendations = []
    if risk_assessment in ["critical", "high"]:
        recommendations.append("ì¦‰ì‹œ ì •ë¹„íŒ€ì— ì—°ë½í•˜ì—¬ ì ê²€ì„ ìš”ì²­í•˜ì„¸ìš”.")
        recommendations.append("í•´ë‹¹ êµ¬ê°„ì˜ ë¬¼ íë¦„ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.")
    if blockage_analysis.blockage_percentage > 30:
        recommendations.append("ì •ê¸°ì ì¸ ì²­ì†Œ ì¼ì •ì„ ì•ë‹¹ê¸°ì„¸ìš”.")
    if avg_confidence < 0.7:
        recommendations.append("ì¹´ë©”ë¼ ë Œì¦ˆë¥¼ ì ê²€í•˜ê³  ì •í™•ë„ë¥¼ ë†’ì´ì„¸ìš”.")
    if len(recent_detections) < 5:
        recommendations.append("ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë¶„ì„ ì •í™•ë„ë¥¼ ë†’ì´ì„¸ìš”.")
    
    # ë™ì  ì¶”ì„¸ ë¶„ì„
    current_risk = current_status.get("risk_score", 0)
    previous_risk = current_status.get("previous_risk_score", current_risk)
    risk_change = current_risk - previous_risk
    
    if risk_change > 5:
        trend_analysis = "ê¸‰ì† ì•…í™”"
    elif risk_change > 2:
        trend_analysis = "ì•…í™”"
    elif risk_change < -5:
        trend_analysis = "ê¸‰ì† ê°œì„ "
    elif risk_change < -2:
        trend_analysis = "ê°œì„ "
    elif abs(risk_change) <= 2:
        trend_analysis = "ì•ˆì •"
    else:
        trend_analysis = "ë¶ˆì•ˆì •"
    
    # ì´ì „ ìœ„í—˜ë„ ì €ì¥
    current_status["previous_risk_score"] = current_risk
    
    return AIAnalysis(
        risk_assessment=risk_assessment,
        confidence_level=min(avg_confidence, 0.95),
        reasoning=reasoning,
        recommendations=recommendations,
        false_positive_probability=false_positive_prob,
        trend_analysis=trend_analysis,
        severity_score=min(severity_score, 100.0)
    )

def update_status(detections_list: List[DetectionData]):
    """ì „ì—­ ìƒíƒœ ì—…ë°ì´íŠ¸ (AI ë¶„ì„ í¬í•¨)"""
    risk_score, ai_analysis = calculate_risk_score_with_ai(detections_list)
    current_status["risk_score"] = risk_score
    current_status["risk_level"] = get_risk_level(risk_score)
    current_status["pipe_status"] = get_pipe_status(current_status["risk_level"])
    current_status["total_detections"] = len(detections_list)
    current_status["ai_analysis"] = ai_analysis.model_dump()
    
    if detections_list:
        current_status["last_detection"] = detections_list[-1].timestamp
        current_status["accumulation_rate"] = len(detections_list)

async def broadcast_to_clients(data: Dict[str, Any]):
    """WebSocket ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
    if not connected_clients:
        return

    message = json.dumps(data, default=str)
    disconnected = []

    for client in connected_clients:
        try:
            await client.send_text(message)
        except Exception as e:
            logger.warning(f"í´ë¼ì´ì–¸íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {e}")
            disconnected.append(client)

    for client in disconnected:
        if client in connected_clients:
            connected_clients.remove(client)

# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.post("/detect", summary="ì“°ë ˆê¸° ê°ì§€ ë°ì´í„° ì²˜ë¦¬")
async def process_detection(data: DetectionData):
    """ì“°ë ˆê¸° ê°ì§€ ë°ì´í„° ì²˜ë¦¬ - ì¦‰ì‹œ ì²˜ë¦¬ ë°©ì‹"""
    try:
        # 1ë‹¨ê³„: ìœ íš¨ì„± ê²€ì‚¬
        if not is_valid_detection(data):
            logger.debug(f"ìœ íš¨í•˜ì§€ ì•Šì€ ê°ì§€ ë¬´ì‹œ: {data.garbage_type} (ì‹ ë¢°ë„: {data.confidence:.2f}, ë©´ì : {data.area})")
            return {
                "success": True,
                "invalid": True,
                "reason": f"Low confidence ({data.confidence:.2f}) or small area ({data.area})",
                "risk_score": current_status["risk_score"],
                "risk_level": current_status["risk_level"]
            }

        # 2ë‹¨ê³„: ì¤‘ë³µ ê°ì§€ í™•ì¸
        if is_duplicate_detection(data):
            logger.debug(f"ì¤‘ë³µ ê°ì§€ ë¬´ì‹œ: {data.garbage_type}")
            return {
                "success": True,
                "duplicate": True,
                "risk_score": current_status["risk_score"],
                "risk_level": current_status["risk_level"]
            }

        # 3ë‹¨ê³„: ì¦‰ì‹œ recent_detectionsì— ì¶”ê°€
        recent_detections.append(data)
        logger.info(f"ğŸ—‘ï¸ ì¦‰ì‹œ ê°ì§€: {data.garbage_type} (ì‹ ë¢°ë„: {data.confidence:.2f}, ë©´ì : {data.area}, ì´ ê°ì§€: {len(recent_detections)})")

        # ìœ„í—˜ë„ ê³„ì‚° ì „ ë¡œê·¸
        logger.info(f"ğŸ” ìœ„í—˜ë„ ê³„ì‚° ì‹œì‘ - í˜„ì¬ ê°ì§€ ìˆ˜: {len(recent_detections)}")

        # 4ë‹¨ê³„: ìƒíƒœ ì—…ë°ì´íŠ¸
        detections_list = list(recent_detections)
        previous_risk_score = current_status.get("risk_score", 0)
        update_status(detections_list)

        previous_level = current_status.get("previous_level", "safe")
        current_level = current_status["risk_level"]
        current_status["previous_level"] = current_level

        # ìœ ì˜ë¯¸í•œ ë³€í™” í™•ì¸ (ë” ë¯¼ê°í•˜ê²Œ)
        risk_change = abs(current_status["risk_score"] - previous_risk_score)
        significant_change = (risk_change >= 5.0) or (current_level != previous_level)  # ë” ë¯¼ê°í•˜ê²Œ ì¡°ì •

        # ì•Œë¦¼ ìƒì„±
        alert = None
        if current_level in ["warning", "caution", "danger"] and current_level != previous_level:
            blockage_info = current_status.get("blockage_percentage", 0)
            flow_restriction = current_status.get("flow_restriction", "ì•Œ ìˆ˜ ì—†ìŒ")
            garbage_volume = current_status.get("garbage_volume", 0)

            # ë ˆë²¨ë³„ ë©”ì‹œì§€ êµ¬ì„±
            if current_level == 'warning':
                emoji = 'âš ï¸'
                level_text = 'ì£¼ì˜ë³´'
            elif current_level == 'caution':
                emoji = 'ğŸŸ '
                level_text = 'ê²½ê³ '
            else:  # danger
                emoji = 'ğŸš¨'
                level_text = 'ìœ„í—˜'

            detailed_message = (
                f"{emoji} í•˜ìˆ˜êµ¬ {level_text}!\n"
                f"â€¢ ë§‰í˜ë¥ : {blockage_info}%\n"
                f"â€¢ íë¦„ ì œí•œ: {flow_restriction}\n"
                f"â€¢ ì¶•ì  ì“°ë ˆê¸°ëŸ‰: {garbage_volume}cmÂ³\n"
                f"â€¢ ìœ„í—˜ë„: {current_status['risk_score']:.1f}%"
            )

            alert = AlertData(
                level=current_level,
                message=detailed_message,
                timestamp=datetime.now(),
                risk_score=current_status['risk_score']
            )

            recent_alerts.appendleft(alert)
            current_status["alerts_today"] += 1
            logger.warning(f"ğŸš¨ ì•Œë¦¼ ë°œìƒ: {detailed_message}")

        # ìœ ì˜ë¯¸í•œ ë³€í™”ì‹œë§Œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        if significant_change:
            broadcast_data = {
                "type": "detection",
                "data": data.model_dump(),
                "status": current_status,
                "alert": alert.model_dump() if alert else None,
                "blockage_analysis": {
                    "blockage_percentage": current_status.get("blockage_percentage", 0),
                    "garbage_volume": current_status.get("garbage_volume", 0),
                    "flow_restriction": current_status.get("flow_restriction", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "accumulated_areas": current_status.get("accumulated_areas", 0)
                },
                "ai_analysis": current_status.get("ai_analysis", {})
            }

            await broadcast_to_clients(broadcast_data)

        # ìœ„í—˜ë„ ê³„ì‚° í›„ ë¡œê·¸
        logger.info(f"ğŸ“Š ìœ„í—˜ë„ ê³„ì‚° ì™„ë£Œ - ì´ì „: {previous_risk_score:.1f}% â†’ í˜„ì¬: {current_status['risk_score']:.1f}% (ë³€í™”: {risk_change:.1f}%)")

        return {
            "success": True,
            "duplicate": False,
            "significant_change": significant_change,
            "risk_score": current_status["risk_score"],
            "risk_level": current_level,
            "blockage_percentage": current_status.get("blockage_percentage", 0),
            "flow_restriction": current_status.get("flow_restriction", "ì•Œ ìˆ˜ ì—†ìŒ"),
            "alert_created": alert is not None
        }

    except Exception as e:
        logger.error(f"ê°ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status", response_model=StatusResponse)
async def get_status():
    """í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    return StatusResponse(**current_status)

@app.get("/blockage-analysis", response_model=BlockageAnalysis)
async def get_blockage_analysis():
    """í˜„ì¬ í•˜ìˆ˜êµ¬ ë§‰í˜ ë¶„ì„ ê²°ê³¼"""
    detections_list = list(recent_detections)
    return analyze_pipe_blockage(detections_list)

@app.get("/detections")
async def get_recent_detections(limit: int = 20):
    """ìµœê·¼ ê°ì§€ ê¸°ë¡ ì¡°íšŒ"""
    detections = list(recent_detections)[-limit:]
    return {
        "detections": [d.model_dump() for d in detections],
        "total": len(recent_detections),
        "limit": limit
    }

@app.get("/alerts")
async def get_recent_alerts(limit: int = 10):
    """ìµœê·¼ ì•Œë¦¼ ì¡°íšŒ"""
    alerts = list(recent_alerts)[:limit]
    return {
        "alerts": [a.model_dump() for a in alerts],
        "total": len(recent_alerts),
        "limit": limit
    }

@app.post("/reset")
async def reset_system():
    """ì‹œìŠ¤í…œ ìƒíƒœ ì´ˆê¸°í™”"""
    global current_status, pending_detections

    recent_detections.clear()
    recent_alerts.clear()
    pending_detections.clear()  # ëŒ€ê¸° ì¤‘ì¸ ê°ì§€ë“¤ë„ ì´ˆê¸°í™”
    confirmed_detections.clear()  # í™•ì •ëœ ê°ì§€ë“¤ë„ ì´ˆê¸°í™”

    current_status = {
        "risk_score": 0.0,
        "risk_level": "safe",
        "total_detections": 0,
        "last_detection": None,
        "alerts_today": 0,
        "pipe_status": "ì •ìƒ - ì›í™œí•œ íë¦„",
        "accumulation_rate": 0.0,
        "blockage_percentage": 0.0,
        "garbage_volume": 0.0,
        "flow_restriction": "ì—†ìŒ",
        "accumulated_areas": 0
    }

    await broadcast_to_clients({
        "type": "reset",
        "status": current_status
    })

    logger.info("ğŸ”„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë“  ê°ì§€ ë°ì´í„° ë° ëŒ€ê¸° ìƒíƒœ ì´ˆê¸°í™”")
    return {"success": True, "message": "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}

# ==================== ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ====================

from fastapi.responses import StreamingResponse

@app.get("/video_feed")
async def video_feed():
    """ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°"""
    def generate():
        while True:
            if current_frame is not None:
                ret, buffer = cv2.imencode('.jpg', current_frame)
                if ret:
                    frame_bytes = buffer.tobytes()
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
            else:
                # ê¸°ë³¸ ì´ë¯¸ì§€
                black_frame = np.zeros((480, 640, 3), dtype=np.uint8)
                cv2.putText(black_frame, 'Camera not active', (200, 240),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                ret, buffer = cv2.imencode('.jpg', black_frame)
                if ret:
                    frame_bytes = buffer.tobytes()
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')

            import time
            time.sleep(0.033)  # ~30 FPS

    return StreamingResponse(generate(), media_type="multipart/x-mixed-replace; boundary=frame")

@app.post("/update_frame")
async def update_frame(frame_data: dict):
    """ì¹´ë©”ë¼ í”„ë ˆì„ ì—…ë°ì´íŠ¸"""
    global current_frame, camera_active

    try:
        frame_base64 = frame_data.get('frame')
        if frame_base64:
            frame_bytes = base64.b64decode(frame_base64)
            nparr = np.frombuffer(frame_bytes, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if frame is not None:
                current_frame = frame
                camera_active = True
                return {"success": True, "message": "Frame updated"}

        return {"success": False, "message": "Invalid frame data"}

    except Exception as e:
        logger.error(f"í”„ë ˆì„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return {"success": False, "message": str(e)}

# ==================== WebSocket ====================

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°"""
    await websocket.accept()
    connected_clients.append(websocket)
    logger.info(f"ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°. ì´ ì—°ê²°: {len(connected_clients)}")

    try:
        # ì—°ê²° ì¦‰ì‹œ í˜„ì¬ ìƒíƒœ ì „ì†¡
        initial_data = {
            "type": "initial",
            "status": current_status,
            "recent_detections": [d.model_dump() for d in list(recent_detections)[-5:]],
            "recent_alerts": [a.model_dump() for a in list(recent_alerts)[:3]],
            "ai_analysis": current_status.get("ai_analysis", {})
        }
        await websocket.send_text(json.dumps(initial_data, default=str))

        # ì—°ê²° ìœ ì§€
        while True:
            message = await websocket.receive_text()
            if message == "ping":
                await websocket.send_text("pong")

    except WebSocketDisconnect:
        if websocket in connected_clients:
            connected_clients.remove(websocket)
        logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œ. ë‚¨ì€ ì—°ê²°: {len(connected_clients)}")
    except Exception as e:
        logger.error(f"WebSocket ì˜¤ë¥˜: {e}")
        if websocket in connected_clients:
            connected_clients.remove(websocket)

# ==================== í—¬ìŠ¤ì²´í¬ ====================

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "connected_clients": len(connected_clients),
        "camera_active": camera_active,
        "total_detections": len(recent_detections),
        "version": "2.0.0"
    }

# ==================== ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ====================

async def periodic_risk_update():
    """ì£¼ê¸°ì ìœ¼ë¡œ ìœ„í—˜ë„ ì—…ë°ì´íŠ¸í•˜ì—¬ ìë™ ê°ì†Œ ì²˜ë¦¬"""
    while True:
        try:
            await asyncio.sleep(5.0)  # 5ì´ˆë§ˆë‹¤ ì‹¤í–‰

            # ìœ„í—˜ë„ ìë™ ê°ì†Œ ì²˜ë¦¬ (ë” ì—„ê²©í•œ ì¡°ê±´)
            current_risk = current_status.get("risk_score", 0)
            time_since_last = get_time_since_last_detection()

            # ì¡°ê±´: ê°ì§€ê°€ ì—†ê³  + ë§ˆì§€ë§‰ ê°ì§€ë¡œë¶€í„° 30ì´ˆ ì´ìƒ ê²½ê³¼ + í˜„ì¬ ìœ„í—˜ë„ê°€ 5ë³´ë‹¤ í° ê²½ìš°
            should_decay = (
                len(recent_detections) == 0 or
                (time_since_last >= 0.5 and current_risk > 5)  # 0.5ë¶„ = 30ì´ˆ
            )

            if should_decay and current_risk > 0:
                previous_risk = current_risk
                previous_level = current_status.get("risk_level", "safe")

                # ìì—° ê°ì†Œ ì ìš© (ë” ì²œì²œíˆ)
                decay_rate = 1.0  # 5ì´ˆë‹¹ 1% ê°ì†Œ (ê¸°ì¡´ë³´ë‹¤ ë” ëŠë¦¼)
                decay_amount = min(decay_rate, current_risk)  # í˜„ì¬ ìœ„í—˜ë„ë¥¼ ë„˜ì§€ ì•Šë„ë¡

                new_risk = max(0, current_risk - decay_amount)
                current_status["risk_score"] = new_risk
                current_status["risk_level"] = get_risk_level(new_risk)
                current_status["pipe_status"] = get_pipe_status(current_status["risk_level"])

                current_level = current_status["risk_level"]

                # ì‹¤ì œë¡œ ê°ì†Œí–ˆê³  ë ˆë²¨ì´ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì•Œë¦¼
                if new_risk < previous_risk and current_level != previous_level:
                    broadcast_data = {
                        "type": "auto_decay",
                        "status": current_status,
                        "message": f"ì“°ë ˆê¸°ê°€ ê°ì§€ë˜ì§€ ì•Šì•„ ìœ„í—˜ë„ê°€ ìë™ìœ¼ë¡œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ({time_since_last:.1f}ë¶„ ê²½ê³¼)"
                    }
                    await broadcast_to_clients(broadcast_data)
                    logger.info(
                        f"ğŸ“‰ ìë™ ìœ„í—˜ë„ ê°ì†Œ: {previous_risk:.1f}% â†’ {new_risk:.1f}% ({current_level}) - {time_since_last:.1f}ë¶„ ê²½ê³¼")

        except Exception as e:
            logger.error(f"ë°±ê·¸ë¼ìš´ë“œ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")

# ==================== ì‹œê°„ ê³„ì‚° í•¨ìˆ˜ ìˆ˜ì • ====================

def get_time_since_last_detection() -> float:
    """ë§ˆì§€ë§‰ ê°ì§€ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„ (ë¶„) - ë” ì •í™•í•œ ê³„ì‚°"""
    if not recent_detections:
        return 60.0  # ê¸°ë³¸ê°’: 60ë¶„ (ê°ì§€ê°€ ì „í˜€ ì—†ìŒ)

    try:
        last_detection = recent_detections[-1]
        last_time = datetime.fromisoformat(last_detection.timestamp.replace('Z', '+00:00'))

        # ì‹œê°„ëŒ€ ì •ë³´ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ëŒ€ë¡œ ê°€ì •
        if last_time.tzinfo is None:
            last_time = last_time.replace(tzinfo=datetime.now().astimezone().tzinfo)

        now = datetime.now().astimezone()
        time_diff = (now - last_time).total_seconds() / 60  # ë¶„ ë‹¨ìœ„

        return max(0.0, time_diff)
    except Exception as e:
        logger.warning(f"ì‹œê°„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 10.0  # ì˜¤ë¥˜ì‹œ ê¸°ë³¸ê°’

# ==================== ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì´ë²¤íŠ¸ ====================

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘"""
    logger.info("ğŸš€ í•˜ìˆ˜ë„ ë§‰í˜ ê°ì§€ ì‹œìŠ¤í…œ ì‹œì‘")
    asyncio.create_task(periodic_risk_update())

# ==================== ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê¹… ì¶”ê°€ ====================

def log_risk_calculation_details(previous_risk: float, new_risk: float, detections_count: int):
    """ìœ„í—˜ë„ ê³„ì‚° ê³¼ì • ìƒì„¸ ë¡œê¹…"""
    logger.debug(f"ìœ„í—˜ë„ ê³„ì‚° ìƒì„¸:")
    logger.debug(f"  - ì´ì „ ìœ„í—˜ë„: {previous_risk:.1f}%")
    logger.debug(f"  - ìƒˆ ìœ„í—˜ë„: {new_risk:.1f}%")
    logger.debug(f"  - ë³€í™”ëŸ‰: {new_risk - previous_risk:+.1f}%")
    logger.debug(f"  - ê°ì§€ ìˆ˜: {detections_count}")
    logger.debug(f"  - ë§ˆì§€ë§‰ ê°ì§€ í›„ ê²½ê³¼ì‹œê°„: {get_time_since_last_detection():.1f}ë¶„")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
"""
í•˜ìˆ˜ë„ ë§‰í˜ ì‹¤ì‹œê°„ ê°ì§€ FastAPI ë°±ì—”ë“œ
ì„œë²„ì™€ í”„ë¡ íŠ¸ì—”ë“œ ë¶„ë¦¬ ë²„ì „
"""

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from collections import deque
from datetime import datetime, timedelta
import asyncio
import json
import logging
import math
import cv2
import base64
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="í•˜ìˆ˜ë„ ë§‰í˜ ê°ì§€ ì‹œìŠ¤í…œ API", version="2.0.0")

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ë¶„ë¦¬ë¡œ ì¸í•´ í•„ìš”)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš© - ì‹¤ì œ ë°°í¬ì‹œì—ëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== ë°ì´í„° ëª¨ë¸ ====================

class DetectionData(BaseModel):
    timestamp: str
    garbage_type: str
    confidence: float
    bbox: List[int]  # [x1, y1, x2, y2]
    area: float
    location: str = "main_pipe"

class AlertData(BaseModel):
    level: str  # "safe", "warning", "danger"
    message: str
    timestamp: datetime
    risk_score: float

class StatusResponse(BaseModel):
    risk_level: str
    risk_score: float
    total_detections: int
    last_detection: Optional[str]
    alerts_today: int
    pipe_status: str
    blockage_percentage: Optional[float] = 0.0
    garbage_volume: Optional[float] = 0.0
    flow_restriction: Optional[str] = "ì—†ìŒ"
    accumulated_areas: Optional[int] = 0

class BlockageAnalysis(BaseModel):
    blockage_percentage: float
    garbage_volume: float
    flow_restriction: str
    accumulated_areas: int
    total_area: float

class AIAnalysis(BaseModel):
    """AI ë¶„ì„ ê²°ê³¼"""
    risk_assessment: str  # "low", "medium", "high", "critical"
    confidence_level: float  # AI ë¶„ì„ ì‹ ë¢°ë„ (0-1)
    reasoning: str  # AI ë¶„ì„ ê·¼ê±°
    recommendations: List[str]  # AI ê¶Œì¥ì‚¬í•­
    false_positive_probability: float  # ì˜¤íƒì§€ í™•ë¥ 
    trend_analysis: str  # ì¶”ì„¸ ë¶„ì„
    severity_score: float  # AI ì‹¬ê°ë„ ì ìˆ˜ (0-100)

# ==================== ì „ì—­ ìƒíƒœ ====================

current_status = {
    "risk_score": 0.0,
    "risk_level": "safe",
    "total_detections": 0,
    "last_detection": None,
    "alerts_today": 0,
    "pipe_status": "ì •ìƒ - ì›í™œí•œ íë¦„",
    "accumulation_rate": 0.0,
    "blockage_percentage": 0.0,
    "garbage_volume": 0.0,
    "flow_restriction": "ì—†ìŒ",
    "accumulated_areas": 0
}

recent_detections: deque = deque(maxlen=100)
recent_alerts: deque = deque(maxlen=50)
connected_clients: List[WebSocket] = []

# ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
current_frame = None
camera_active = False

# ìœ„í—˜ë„ ì„ê³„ê°’ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì •)
RISK_THRESHOLDS = {
    "safe": (0, 60),      # 60% ë¯¸ë§Œ: ì•ˆì „ (ë” ê´€ëŒ€í•˜ê²Œ)
    "warning": (60, 80),  # 60-80%: ì£¼ì˜ (ë²”ìœ„ ì¶•ì†Œ)
    "danger": (80, 100)   # 80% ì´ìƒ: ìœ„í—˜ (ë” ì—„ê²©í•˜ê²Œ)
}

# AI ë¶„ì„ ê°€ì¤‘ì¹˜ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
AI_WEIGHTS = {
    "low": 0.2,           # AIê°€ ë‚®ì€ ìœ„í—˜ìœ¼ë¡œ íŒë‹¨ì‹œ ê°€ì¤‘ì¹˜ ëŒ€í­ ê°ì†Œ
    "medium": 0.5,        # AIê°€ ì¤‘ê°„ ìœ„í—˜ìœ¼ë¡œ íŒë‹¨ì‹œ ê°€ì¤‘ì¹˜ ê°ì†Œ
    "high": 0.8,          # AIê°€ ë†’ì€ ìœ„í—˜ìœ¼ë¡œ íŒë‹¨ì‹œ ê°€ì¤‘ì¹˜ ì œí•œ
    "critical": 1.0       # AIê°€ ë§¤ìš° ìœ„í—˜ìœ¼ë¡œ íŒë‹¨ì‹œë„ ë³´ìˆ˜ì  ì ‘ê·¼
}

# ê°ì§€ ì‹ ë¢°ë„ ì„ê³„ê°’ (ë” ì—„ê²©í•˜ê²Œ)
MIN_CONFIDENCE_THRESHOLD = 0.7  # 70% ì´ìƒ ì‹ ë¢°ë„ë§Œ ì²˜ë¦¬ (ë” ì—„ê²©)
MIN_AREA_THRESHOLD = 2000       # ìµœì†Œ ë©´ì  ì¦ê°€ (ë” í° ê°ì²´ë§Œ)
MIN_DETECTIONS_FOR_WARNING = 5  # ê²½ê³  ë°œìƒì„ ìœ„í•œ ìµœì†Œ ê°ì§€ íšŸìˆ˜ ì¦ê°€
MIN_DETECTIONS_FOR_DANGER = 8   # ìœ„í—˜ ë°œìƒì„ ìœ„í•œ ìµœì†Œ ê°ì§€ íšŸìˆ˜

# ==================== ë¶„ì„ í•¨ìˆ˜ë“¤ ====================

def is_duplicate_detection(new_detection: DetectionData, threshold_seconds: int = 10) -> bool:
    """ì¤‘ë³µ ê°ì§€ì¸ì§€ í™•ì¸"""
    if not recent_detections:
        return False
    
    try:
        new_time = datetime.fromisoformat(new_detection.timestamp.replace('Z', '+00:00'))
        new_bbox = new_detection.bbox
        new_center = ((new_bbox[0] + new_bbox[2]) // 2, (new_bbox[1] + new_bbox[3]) // 2)
        
        for detection in list(recent_detections)[-5:]:
            try:
                det_time = datetime.fromisoformat(detection.timestamp.replace('Z', '+00:00'))
                time_diff = (new_time - det_time).total_seconds()
                
                if time_diff < threshold_seconds:
                    det_bbox = detection.bbox
                    det_center = ((det_bbox[0] + det_bbox[2]) // 2, (det_bbox[1] + det_bbox[3]) // 2)
                    distance = math.sqrt((new_center[0] - det_center[0])**2 + (new_center[1] - det_center[1])**2)
                    
                    if distance < 50 and detection.garbage_type == new_detection.garbage_type:
                        return True
            except:
                continue
                
    except:
        pass
    
    return False

def analyze_pipe_blockage(detections: List[DetectionData]) -> BlockageAnalysis:
    """í•˜ìˆ˜êµ¬ ë§‰í˜ ì •ë„ ë¶„ì„"""
    if not detections:
        return BlockageAnalysis(
            blockage_percentage=0.0,
            garbage_volume=0.0,
            flow_restriction="ì—†ìŒ",
            accumulated_areas=0,
            total_area=0
        )
    
    now = datetime.now()
    recent_hour = now - timedelta(hours=1)
    
    # ìµœê·¼ 1ì‹œê°„ ë‚´ ê°ì§€ í•„í„°ë§
    recent_detections_list = []
    for d in detections:
        try:
            detection_time = datetime.fromisoformat(d.timestamp.replace('Z', '+00:00'))
            if detection_time >= recent_hour:
                recent_detections_list.append(d)
        except:
            recent_detections_list.append(d)
    
    # ì¶•ì  ì˜ì—­ ë¶„ì„
    blockage_areas = {}
    total_area = 0
    
    for detection in recent_detections_list:
        bbox = detection.bbox
        area_key = f"{bbox[0]//100}_{bbox[1]//100}"
        
        if area_key not in blockage_areas:
            blockage_areas[area_key] = {
                "count": 0,
                "total_area": 0,
                "max_confidence": 0,
                "garbage_types": set()
            }
        
        blockage_areas[area_key]["count"] += 1
        blockage_areas[area_key]["total_area"] += detection.area
        blockage_areas[area_key]["max_confidence"] = max(
            blockage_areas[area_key]["max_confidence"], 
            detection.confidence
        )
        blockage_areas[area_key]["garbage_types"].add(detection.garbage_type)
        total_area += detection.area
    
    # ë§‰í˜ ì •ë„ ê³„ì‚°
    pipe_width = 640
    pipe_height = 480
    total_pipe_area = pipe_width * pipe_height
    
    blockage_percentage = min((total_area / total_pipe_area) * 100, 100)
    
    # íë¦„ ì œí•œ ìˆ˜ì¤€ ê²°ì •
    if blockage_percentage < 10:
        flow_restriction = "ë¯¸ë¯¸í•¨"
    elif blockage_percentage < 30:
        flow_restriction = "ê²½ë¯¸í•¨"
    elif blockage_percentage < 60:
        flow_restriction = "ë³´í†µ"
    elif blockage_percentage < 80:
        flow_restriction = "ì‹¬ê°í•¨"
    else:
        flow_restriction = "ë§¤ìš° ì‹¬ê°í•¨"
    
    # ìš©ì  ì¶”ì •
    estimated_depth = 5  # cm
    garbage_volume = (total_area / 10000) * estimated_depth  # cmÂ³
    
    return BlockageAnalysis(
        blockage_percentage=round(blockage_percentage, 1),
        garbage_volume=round(garbage_volume, 2),
        flow_restriction=flow_restriction,
        accumulated_areas=len(blockage_areas),
        total_area=total_area
    )

def is_valid_detection(detection: DetectionData) -> bool:
    """ê°ì§€ê°€ ìœ íš¨í•œì§€ ê²€ì‚¬"""
    # ì‹ ë¢°ë„ ì²´í¬
    if detection.confidence < MIN_CONFIDENCE_THRESHOLD:
        return False
    
    # ìµœì†Œ ë©´ì  ì²´í¬
    if detection.area < MIN_AREA_THRESHOLD:
        return False
    
    # ë°”ìš´ë”© ë°•ìŠ¤ ìœ íš¨ì„± ì²´í¬
    bbox = detection.bbox
    if len(bbox) != 4 or bbox[2] <= bbox[0] or bbox[3] <= bbox[1]:
        return False
    
    return True

def calculate_risk_score_with_ai(detections: List[DetectionData]) -> tuple[float, AIAnalysis]:
    """AI ë¶„ì„ì„ í¬í•¨í•œ ì‹ ë¢°ì„± ë†’ì€ ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°"""
    current_risk = current_status.get("risk_score", 0)
    
    if not detections:
        # ì“°ë ˆê¸°ê°€ ì—†ìœ¼ë©´ ìœ„í—˜ë„ ìì—° ê°ì†Œ (ë” ë¹ ë¥´ê²Œ)
        decay_rate = 2.0  # ë¶„ë‹¹ 2% ê°ì†Œ
        time_since_last = get_time_since_last_detection()
        decay_amount = min(decay_rate * time_since_last, current_risk)
        base_score = max(0, current_risk - decay_amount)
        
        ai_analysis = AIAnalysis(
            risk_assessment="low",
            confidence_level=0.9,
            reasoning="ê°ì§€ëœ ì“°ë ˆê¸°ê°€ ì—†ì–´ ì•ˆì „í•œ ìƒíƒœì…ë‹ˆë‹¤. ìœ„í—˜ë„ê°€ ìì—°ì ìœ¼ë¡œ ê°ì†Œí•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            recommendations=["ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê³„ì†í•˜ì„¸ìš”."],
            false_positive_probability=0.0,
            trend_analysis="ê°œì„ ",
            severity_score=0.0
        )
        return base_score, ai_analysis
    
    # ìœ íš¨í•œ ê°ì§€ë§Œ í•„í„°ë§
    valid_detections = [d for d in detections if is_valid_detection(d)]
    
    if not valid_detections:
        base_score = max(0, current_status.get("risk_score", 0) - 1)
        ai_analysis = AIAnalysis(
            risk_assessment="low",
            confidence_level=0.7,
            reasoning="ìœ íš¨í•œ ê°ì§€ê°€ ì—†ì–´ ì•ˆì „í•œ ìƒíƒœì…ë‹ˆë‹¤.",
            recommendations=["ì¹´ë©”ë¼ ì‹œìŠ¤í…œì„ ì ê²€í•˜ì„¸ìš”."],
            false_positive_probability=0.5,
            trend_analysis="ë¶ˆí™•ì‹¤",
            severity_score=0.0
        )
        return base_score, ai_analysis
    
    # AI ë¶„ì„ ìˆ˜í–‰
    blockage_analysis = analyze_pipe_blockage(valid_detections)
    ai_analysis = analyze_with_ai(valid_detections, blockage_analysis)
    
    # ë™ì  ìœ„í—˜ë„ ë³€í™” ê³„ì‚°
    dynamic_change = calculate_dynamic_risk_change(valid_detections, current_risk)
    
    # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚° (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
    blockage_score = blockage_analysis.blockage_percentage * 0.3  # ë§‰í˜ë¥  ê°€ì¤‘ì¹˜ ë” ê°ì†Œ
    volume_score = min(blockage_analysis.garbage_volume / 40, 8)  # ìš©ì  ì ìˆ˜ ë” ê°ì†Œ
    areas_score = min(blockage_analysis.accumulated_areas * 1.5, 6)  # ì˜ì—­ ì ìˆ˜ ë” ê°ì†Œ
    
    # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜
    avg_confidence = sum(d.confidence for d in valid_detections) / len(valid_detections)
    confidence_multiplier = min(avg_confidence / 0.8, 1.0)  # 80% ì´ìƒì¼ ë•Œ ìµœëŒ€ ë°°ìœ¨
    
    # AI ë¶„ì„ ê°€ì¤‘ì¹˜ ì ìš©
    ai_weight = AI_WEIGHTS.get(ai_analysis.risk_assessment, 1.0)
    
    # ì˜¤íƒì§€ í™•ë¥ ì„ ê³ ë ¤í•œ ë³´ì •
    false_positive_correction = 1.0 - (ai_analysis.false_positive_probability * 0.5)
    
    # ìµœì†Œ ê°ì§€ ê°œìˆ˜ ì²´í¬ (ë” ì—„ê²©í•˜ê²Œ)
    if len(valid_detections) < MIN_DETECTIONS_FOR_WARNING:
        base_score = min(15, current_risk)  # ë” ë‚®ì€ ì œí•œ
    elif len(valid_detections) < MIN_DETECTIONS_FOR_DANGER and ai_analysis.risk_assessment in ["low", "medium"]:
        base_score = min(25, current_risk)  # ìœ„í—˜ë„ ì œí•œ
    else:
        base_score = blockage_score + volume_score + areas_score
    
    # AI ë¶„ì„ì´ ë‚®ì€ ìœ„í—˜ìœ¼ë¡œ íŒë‹¨í•˜ë©´ ì ìˆ˜ ê°ì†Œ (ë” ì—„ê²©í•˜ê²Œ)
    if ai_analysis.risk_assessment == "low":
        final_score = base_score * 0.1 * ai_weight * confidence_multiplier * false_positive_correction
    elif ai_analysis.risk_assessment == "medium":
        final_score = base_score * 0.3 * ai_weight * confidence_multiplier * false_positive_correction
    elif ai_analysis.risk_assessment == "high":
        final_score = base_score * 0.6 * ai_weight * confidence_multiplier * false_positive_correction
    else:  # critical
        final_score = base_score * 0.8 * ai_weight * confidence_multiplier * false_positive_correction
    
    # AI ì‹¬ê°ë„ ì ìˆ˜ì™€ ê²°í•© (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
    combined_score = (final_score * 0.4) + (ai_analysis.severity_score * 0.2)
    
    # ë™ì  ë³€í™” ì ìš© (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
    if dynamic_change < 0:  # ê°ì†Œí•˜ëŠ” ê²½ìš°
        # ê°ì†ŒëŸ‰ì„ ë” í¬ê²Œ ì ìš©
        combined_score = max(0, current_risk + dynamic_change * 2.0)
    else:  # ì¦ê°€í•˜ëŠ” ê²½ìš°
        # ì¦ê°€ëŸ‰ì„ ë” ì œí•œì ìœ¼ë¡œ ì ìš©
        combined_score = min(75, current_risk + dynamic_change * 0.5)
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    current_status.update({
        "blockage_percentage": blockage_analysis.blockage_percentage,
        "garbage_volume": blockage_analysis.garbage_volume,
        "flow_restriction": blockage_analysis.flow_restriction,
        "accumulated_areas": blockage_analysis.accumulated_areas,
        "ai_analysis": ai_analysis.model_dump()
    })
    
    # ì ìˆ˜ ì œí•œ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
    return min(combined_score, 70), ai_analysis  # ìµœëŒ€ 70%ë¡œ ì œí•œ

def calculate_risk_score(detections: List[DetectionData]) -> float:
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    score, _ = calculate_risk_score_with_ai(detections)
    return score

def get_risk_level(score: float) -> str:
    """ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •"""
    if score >= RISK_THRESHOLDS["danger"][0]:
        return "danger"
    elif score >= RISK_THRESHOLDS["warning"][0]:
        return "warning"
    else:
        return "safe"

def get_pipe_status(risk_level: str) -> str:
    """íŒŒì´í”„ ìƒíƒœ í…ìŠ¤íŠ¸"""
    status_map = {
        "safe": "ì •ìƒ - ì›í™œí•œ íë¦„",
        "warning": "ì£¼ì˜ - ì¶•ì ëŸ‰ ì¦ê°€",
        "danger": "ìœ„í—˜ - ë§‰í˜ ê°€ëŠ¥ì„± ë†’ìŒ"
    }
    return status_map.get(risk_level, "ì•Œ ìˆ˜ ì—†ìŒ")

def get_time_since_last_detection() -> float:
    """ë§ˆì§€ë§‰ ê°ì§€ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„ (ë¶„)"""
    if not recent_detections:
        return 10.0  # ê¸°ë³¸ê°’: 10ë¶„
    
    try:
        last_detection = recent_detections[-1]
        last_time = datetime.fromisoformat(last_detection.timestamp.replace('Z', '+00:00'))
        now = datetime.now()
        time_diff = (now - last_time).total_seconds() / 60  # ë¶„ ë‹¨ìœ„
        return max(0.1, time_diff)  # ìµœì†Œ 0.1ë¶„
    except:
        return 5.0  # ì˜¤ë¥˜ì‹œ ê¸°ë³¸ê°’

def calculate_dynamic_risk_change(detections: List[DetectionData], current_risk: float) -> float:
    """ë™ì  ìœ„í—˜ë„ ë³€í™” ê³„ì‚°"""
    if not detections:
        # ì“°ë ˆê¸°ê°€ ì—†ìœ¼ë©´ ìì—° ê°ì†Œ
        time_since_last = get_time_since_last_detection()
        decay_rate = 1.5  # ë¶„ë‹¹ 1.5% ê°ì†Œ
        decay_amount = min(decay_rate * time_since_last, current_risk)
        return -decay_amount
    
    # ìµœê·¼ ê°ì§€ ë¶„ì„ (ìµœê·¼ 5ë¶„)
    now = datetime.now()
    recent_detections_5min = []
    
    for d in detections[-10:]:  # ìµœê·¼ 10ê°œë§Œ í™•ì¸
        try:
            det_time = datetime.fromisoformat(d.timestamp.replace('Z', '+00:00'))
            minutes_ago = (now - det_time).total_seconds() / 60
            if minutes_ago <= 5:  # 5ë¶„ ì´ë‚´
                recent_detections_5min.append(d)
        except:
            continue
    
    if not recent_detections_5min:
        # ìµœê·¼ 5ë¶„ ë‚´ ê°ì§€ê°€ ì—†ìœ¼ë©´ ê°ì†Œ
        time_since_last = get_time_since_last_detection()
        decay_rate = 1.0  # ë¶„ë‹¹ 1% ê°ì†Œ
        decay_amount = min(decay_rate * time_since_last, current_risk)
        return -decay_amount
    
    # ìµœê·¼ ê°ì§€ê°€ ìˆìœ¼ë©´ ìœ„í—˜ë„ ì¦ê°€ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
    recent_count = len(recent_detections_5min)
    avg_confidence = sum(d.confidence for d in recent_detections_5min) / len(recent_detections_5min)
    
    # ì‹ ë¢°ë„ê°€ ë†’ê³  ê°ì§€ê°€ ë§ì„ìˆ˜ë¡ ìœ„í—˜ë„ ì¦ê°€ (ë” ì œí•œì ìœ¼ë¡œ)
    increase_rate = min(recent_count * 2 * avg_confidence, 8)  # ìµœëŒ€ 8% ì¦ê°€
    
    return increase_rate

def analyze_with_ai(detections: List[DetectionData], blockage_analysis: BlockageAnalysis) -> AIAnalysis:
    """AI ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ì„"""
    if not detections:
        return AIAnalysis(
            risk_assessment="low",
            confidence_level=0.9,
            reasoning="ê°ì§€ëœ ì“°ë ˆê¸°ê°€ ì—†ì–´ ì•ˆì „í•œ ìƒíƒœì…ë‹ˆë‹¤.",
            recommendations=["ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê³„ì†í•˜ì„¸ìš”."],
            false_positive_probability=0.0,
            trend_analysis="ì•ˆì •ì ",
            severity_score=0.0
        )
    
    # ìµœê·¼ ê°ì§€ ë¶„ì„ (ìµœê·¼ 10ê°œ)
    recent_detections = detections[-10:] if len(detections) > 10 else detections
    now = datetime.now()
    
    # ì‹œê°„ ë¶„í¬ ë¶„ì„
    time_distribution = []
    for d in recent_detections:
        try:
            det_time = datetime.fromisoformat(d.timestamp.replace('Z', '+00:00'))
            minutes_ago = (now - det_time).total_seconds() / 60
            time_distribution.append(minutes_ago)
        except:
            continue
    
    # ì‹ ë¢°ë„ ë¶„ì„
    avg_confidence = sum(d.confidence for d in recent_detections) / len(recent_detections)
    high_confidence_count = sum(1 for d in recent_detections if d.confidence > 0.8)
    confidence_ratio = high_confidence_count / len(recent_detections)
    
    # ì“°ë ˆê¸° ìœ í˜• ë¶„ì„
    garbage_types = {}
    for d in recent_detections:
        garbage_types[d.garbage_type] = garbage_types.get(d.garbage_type, 0) + 1
    
    # AI ë¶„ì„ ë¡œì§
    risk_factors = []
    severity_score = 0.0
    
    # ë™ì  ë³€í™” ë¶„ì„
    current_risk = current_status.get("risk_score", 0)
    previous_risk = current_status.get("previous_risk_score", current_risk)
    risk_change = current_risk - previous_risk
    
    # 1. ë§‰í˜ë¥  ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if blockage_analysis.blockage_percentage > 80:
        risk_factors.append("ë§‰í˜ë¥ ì´ 80%ë¥¼ ì´ˆê³¼í•˜ì—¬ ë§¤ìš° ì‹¬ê°í•œ ìƒí™©")
        severity_score += 40
    elif blockage_analysis.blockage_percentage > 60:
        risk_factors.append("ë§‰í˜ë¥ ì´ 60%ë¥¼ ì´ˆê³¼í•˜ì—¬ ì‹¬ê°í•œ ìƒí™©")
        severity_score += 25
    elif blockage_analysis.blockage_percentage > 40:
        risk_factors.append("ë§‰í˜ë¥ ì´ 40%ë¥¼ ì´ˆê³¼í•˜ì—¬ ì£¼ì˜ê°€ í•„ìš”")
        severity_score += 10
    elif blockage_analysis.blockage_percentage > 20:
        risk_factors.append("ë§‰í˜ë¥ ì´ 20%ë¥¼ ì´ˆê³¼í•˜ì—¬ ëª¨ë‹ˆí„°ë§ í•„ìš”")
        severity_score += 5
    
    # 2. ì‹ ë¢°ë„ ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if avg_confidence < 0.8:
        risk_factors.append("í‰ê·  ì‹ ë¢°ë„ê°€ ë‚®ì•„ ì˜¤íƒì§€ ê°€ëŠ¥ì„± ë†’ìŒ")
        severity_score -= 15  # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ìœ„í—˜ë„ ëŒ€í­ ê°ì†Œ
    elif avg_confidence > 0.95:
        risk_factors.append("ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„ë¡œ ì •í™•í•œ ê°ì§€")
        severity_score += 8
    elif avg_confidence > 0.85:
        risk_factors.append("ë†’ì€ ì‹ ë¢°ë„ë¡œ ì •í™•í•œ ê°ì§€")
        severity_score += 3
    
    # 3. ì‹œê°„ ë¶„í¬ ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if time_distribution:
        recent_count = sum(1 for t in time_distribution if t <= 5)  # 5ë¶„ ì´ë‚´
        if recent_count >= 5:
            risk_factors.append("ìµœê·¼ 5ë¶„ ë‚´ ë‹¤ìˆ˜ ê°ì§€ë¡œ ê¸‰ì†í•œ ì¶•ì ")
            severity_score += 25
        elif recent_count >= 3:
            risk_factors.append("ìµœê·¼ 5ë¶„ ë‚´ ì—¬ëŸ¬ ê°ì§€ë¡œ ì¶•ì  ì¦ê°€")
            severity_score += 15
        elif recent_count >= 1:
            risk_factors.append("ìµœê·¼ ê°ì§€ë¡œ ì§€ì†ì  ëª¨ë‹ˆí„°ë§ í•„ìš”")
            severity_score += 5
    
    # 4. ì“°ë ˆê¸° ìœ í˜• ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    dangerous_types = ["plastic_bag", "cloth", "paper", "organic"]
    dangerous_count = sum(garbage_types.get(t, 0) for t in dangerous_types)
    if dangerous_count >= 5:
        risk_factors.append("ë§‰í˜ ìœ„í—˜ì´ ë†’ì€ ì“°ë ˆê¸° ë‹¤ìˆ˜ ê°ì§€")
        severity_score += 20
    elif dangerous_count >= 3:
        risk_factors.append("ë§‰í˜ ìœ„í—˜ì´ ë†’ì€ ì“°ë ˆê¸° ì—¬ëŸ¬ ê°œ ê°ì§€")
        severity_score += 10
    
    # 5. ì¶•ì  ì˜ì—­ ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if blockage_analysis.accumulated_areas >= 8:
        risk_factors.append("ë‹¤ìˆ˜ì˜ ì¶•ì  ì˜ì—­ìœ¼ë¡œ ë¶„ì‚°ëœ ë§‰í˜")
        severity_score += 15
    elif blockage_analysis.accumulated_areas >= 5:
        risk_factors.append("ì—¬ëŸ¬ ì¶•ì  ì˜ì—­ìœ¼ë¡œ ë§‰í˜ ìœ„í—˜")
        severity_score += 8
    
    # 6. ë™ì  ë³€í™” ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if risk_change > 10:
        risk_factors.append("ìœ„í—˜ë„ê°€ ê¸‰ì†íˆ ì¦ê°€í•˜ëŠ” ìƒí™©")
        severity_score += 20
    elif risk_change > 5:
        risk_factors.append("ìœ„í—˜ë„ê°€ ì ì§„ì ìœ¼ë¡œ ì¦ê°€")
        severity_score += 10
    elif risk_change < -10:
        risk_factors.append("ìœ„í—˜ë„ê°€ ê¸‰ì†íˆ ê°ì†Œí•˜ëŠ” ê°œì„  ìƒí™©")
        severity_score -= 15
    elif risk_change < -5:
        risk_factors.append("ìœ„í—˜ë„ê°€ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ")
        severity_score -= 8
    
    # 7. ì¶”ì„¸ ë¶„ì„ (ë” ì—„ê²©í•˜ê²Œ)
    if len(detections) >= 30:  # ë” ë§ì€ ë°ì´í„° í•„ìš”
        recent_15 = detections[-15:]
        older_15 = detections[-30:-15]
        recent_avg = sum(d.area for d in recent_15) / len(recent_15)
        older_avg = sum(d.area for d in older_15) / len(older_15)
        
        if recent_avg > older_avg * 2.0:  # ë” í° ì¦ê°€ í•„ìš”
            risk_factors.append("ì“°ë ˆê¸° í¬ê¸°ê°€ ê¸‰ì†íˆ ì¦ê°€í•˜ëŠ” ì¶”ì„¸")
            severity_score += 15
        elif recent_avg > older_avg * 1.5:
            risk_factors.append("ì“°ë ˆê¸° í¬ê¸°ê°€ ì¦ê°€í•˜ëŠ” ì¶”ì„¸")
            severity_score += 8
        elif recent_avg < older_avg * 0.3:  # ë” í° ê°ì†Œ í•„ìš”
            risk_factors.append("ì“°ë ˆê¸° í¬ê¸°ê°€ ê¸‰ì†íˆ ê°ì†Œí•˜ëŠ” ì¶”ì„¸")
            severity_score -= 10
        elif recent_avg < older_avg * 0.5:
            risk_factors.append("ì“°ë ˆê¸° í¬ê¸°ê°€ ê°ì†Œí•˜ëŠ” ì¶”ì„¸")
            severity_score -= 5
    
    # ì˜¤íƒì§€ í™•ë¥  ê³„ì‚° (ë” ì—„ê²©í•˜ê²Œ)
    false_positive_prob = 0.0
    if avg_confidence < 0.8:
        false_positive_prob = 0.4
    if len(recent_detections) < 5:
        false_positive_prob += 0.3
    if len(recent_detections) < 3:
        false_positive_prob += 0.2
    
    # ìœ„í—˜ë„ í‰ê°€ (ë” ì—„ê²©í•˜ê²Œ)
    if severity_score >= 70:  # ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
        risk_assessment = "critical"
        if risk_change > 10:
            reasoning = "ë‹¤ì¤‘ ìœ„í—˜ ìš”ì†Œê°€ ë³µí•©ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ë§¤ìš° ìœ„í—˜í•œ ìƒí™©ì´ë©°, ìœ„í—˜ë„ê°€ ê¸‰ì†íˆ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        else:
            reasoning = "ë‹¤ì¤‘ ìœ„í—˜ ìš”ì†Œê°€ ë³µí•©ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ë§¤ìš° ìœ„í—˜í•œ ìƒí™©"
    elif severity_score >= 50:  # ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
        risk_assessment = "high"
        if risk_change > 5:
            reasoning = "ì—¬ëŸ¬ ìœ„í—˜ ìš”ì†Œê°€ í™•ì¸ë˜ì–´ ë†’ì€ ìœ„í—˜ë„ì´ë©°, ìœ„í—˜ë„ê°€ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ì…ë‹ˆë‹¤."
        else:
            reasoning = "ì—¬ëŸ¬ ìœ„í—˜ ìš”ì†Œê°€ í™•ì¸ë˜ì–´ ë†’ì€ ìœ„í—˜ë„"
    elif severity_score >= 25:  # ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
        risk_assessment = "medium"
        if risk_change < -5:
            reasoning = "ì¼ë¶€ ìœ„í—˜ ìš”ì†Œê°€ í™•ì¸ë˜ì–´ ì£¼ì˜ê°€ í•„ìš”í•˜ì§€ë§Œ, ìœ„í—˜ë„ê°€ ê°ì†Œí•˜ëŠ” ê°œì„  ì¶”ì„¸ì…ë‹ˆë‹¤."
        else:
            reasoning = "ì¼ë¶€ ìœ„í—˜ ìš”ì†Œê°€ í™•ì¸ë˜ì–´ ì£¼ì˜ê°€ í•„ìš”"
    else:
        risk_assessment = "low"
        if risk_change < -10:
            reasoning = "ëŒ€ë¶€ë¶„ì˜ ì§€í‘œê°€ ì•ˆì „ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©°, ìœ„í—˜ë„ê°€ ê¸‰ì†íˆ ê°ì†Œí•˜ëŠ” ì¢‹ì€ ìƒí™©ì…ë‹ˆë‹¤."
        elif risk_change < -5:
            reasoning = "ëŒ€ë¶€ë¶„ì˜ ì§€í‘œê°€ ì•ˆì „ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©°, ìœ„í—˜ë„ê°€ ê°ì†Œí•˜ëŠ” ê°œì„  ì¶”ì„¸ì…ë‹ˆë‹¤."
        else:
            reasoning = "ëŒ€ë¶€ë¶„ì˜ ì§€í‘œê°€ ì•ˆì „ ë²”ìœ„ ë‚´ì— ìˆìŒ"
    
    # ê¶Œì¥ì‚¬í•­ ìƒì„±
    recommendations = []
    if risk_assessment in ["critical", "high"]:
        recommendations.append("ì¦‰ì‹œ ì •ë¹„íŒ€ì— ì—°ë½í•˜ì—¬ ì ê²€ì„ ìš”ì²­í•˜ì„¸ìš”.")
        recommendations.append("í•´ë‹¹ êµ¬ê°„ì˜ ë¬¼ íë¦„ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.")
    if blockage_analysis.blockage_percentage > 30:
        recommendations.append("ì •ê¸°ì ì¸ ì²­ì†Œ ì¼ì •ì„ ì•ë‹¹ê¸°ì„¸ìš”.")
    if avg_confidence < 0.7:
        recommendations.append("ì¹´ë©”ë¼ ë Œì¦ˆë¥¼ ì ê²€í•˜ê³  ì •í™•ë„ë¥¼ ë†’ì´ì„¸ìš”.")
    if len(recent_detections) < 5:
        recommendations.append("ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë¶„ì„ ì •í™•ë„ë¥¼ ë†’ì´ì„¸ìš”.")
    
    # ë™ì  ì¶”ì„¸ ë¶„ì„
    current_risk = current_status.get("risk_score", 0)
    previous_risk = current_status.get("previous_risk_score", current_risk)
    risk_change = current_risk - previous_risk
    
    if risk_change > 5:
        trend_analysis = "ê¸‰ì† ì•…í™”"
    elif risk_change > 2:
        trend_analysis = "ì•…í™”"
    elif risk_change < -5:
        trend_analysis = "ê¸‰ì† ê°œì„ "
    elif risk_change < -2:
        trend_analysis = "ê°œì„ "
    elif abs(risk_change) <= 2:
        trend_analysis = "ì•ˆì •"
    else:
        trend_analysis = "ë¶ˆì•ˆì •"
    
    # ì´ì „ ìœ„í—˜ë„ ì €ì¥
    current_status["previous_risk_score"] = current_risk
    
    return AIAnalysis(
        risk_assessment=risk_assessment,
        confidence_level=min(avg_confidence, 0.95),
        reasoning=reasoning,
        recommendations=recommendations,
        false_positive_probability=false_positive_prob,
        trend_analysis=trend_analysis,
        severity_score=min(severity_score, 100.0)
    )

def update_status(detections_list: List[DetectionData]):
    """ì „ì—­ ìƒíƒœ ì—…ë°ì´íŠ¸ (AI ë¶„ì„ í¬í•¨)"""
    risk_score, ai_analysis = calculate_risk_score_with_ai(detections_list)
    current_status["risk_score"] = risk_score
    current_status["risk_level"] = get_risk_level(risk_score)
    current_status["pipe_status"] = get_pipe_status(current_status["risk_level"])
    current_status["total_detections"] = len(detections_list)
    current_status["ai_analysis"] = ai_analysis.model_dump()
    
    if detections_list:
        current_status["last_detection"] = detections_list[-1].timestamp
        current_status["accumulation_rate"] = len(detections_list)

async def broadcast_to_clients(data: Dict[str, Any]):
    """WebSocket ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
    if not connected_clients:
        return
    
    message = json.dumps(data, default=str)
    disconnected = []
    
    for client in connected_clients:
        try:
            await client.send_text(message)
        except Exception as e:
            logger.warning(f"í´ë¼ì´ì–¸íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {e}")
            disconnected.append(client)
    
    for client in disconnected:
        if client in connected_clients:
            connected_clients.remove(client)

# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.post("/detect", summary="ì“°ë ˆê¸° ê°ì§€ ë°ì´í„° ì²˜ë¦¬")
async def process_detection(data: DetectionData):
    """ì“°ë ˆê¸° ê°ì§€ ë°ì´í„° ì²˜ë¦¬ - ì‹ ë¢°ì„± ê²€ì¦ ë° ì¤‘ë³µ ë°©ì§€"""
    try:
        # 1ë‹¨ê³„: ìœ íš¨ì„± ê²€ì‚¬
        if not is_valid_detection(data):
            logger.debug(f"ìœ íš¨í•˜ì§€ ì•Šì€ ê°ì§€ ë¬´ì‹œ: {data.garbage_type} (ì‹ ë¢°ë„: {data.confidence:.2f}, ë©´ì : {data.area})")
            return {
                "success": True,
                "invalid": True,
                "reason": f"Low confidence ({data.confidence:.2f}) or small area ({data.area})",
                "risk_score": current_status["risk_score"],
                "risk_level": current_status["risk_level"]
            }
        
        # 2ë‹¨ê³„: ì¤‘ë³µ ê°ì§€ í™•ì¸
        if is_duplicate_detection(data):
            logger.debug(f"ì¤‘ë³µ ê°ì§€ ë¬´ì‹œ: {data.garbage_type}")
            return {
                "success": True,
                "duplicate": True,
                "risk_score": current_status["risk_score"],
                "risk_level": current_status["risk_level"]
            }
        
        # 3ë‹¨ê³„: ìƒˆë¡œìš´ ê°ì§€ ì €ì¥
        recent_detections.append(data)
        logger.info(f"ğŸ—‘ï¸ ìœ íš¨í•œ ì“°ë ˆê¸° ê°ì§€: {data.garbage_type} (ì‹ ë¢°ë„: {data.confidence:.2f}, ë©´ì : {data.area})")
        
        # 4ë‹¨ê³„: ìƒíƒœ ì—…ë°ì´íŠ¸
        detections_list = list(recent_detections)
        previous_risk_score = current_status.get("risk_score", 0)
        update_status(detections_list)
        
        previous_level = current_status.get("previous_level", "safe")
        current_level = current_status["risk_level"]
        current_status["previous_level"] = current_level
        
        # 5ë‹¨ê³„: ìœ ì˜ë¯¸í•œ ë³€í™” í™•ì¸ (ë” ì—„ê²©í•˜ê²Œ)
        risk_change = abs(current_status["risk_score"] - previous_risk_score)
        significant_change = (risk_change >= 10.0) or (current_level != previous_level and current_level != "safe")
        
        # ì•Œë¦¼ ìƒì„±
        alert = None
        if current_level in ["warning", "danger"] and current_level != previous_level:
            blockage_info = current_status.get("blockage_percentage", 0)
            flow_restriction = current_status.get("flow_restriction", "ì•Œ ìˆ˜ ì—†ìŒ")
            garbage_volume = current_status.get("garbage_volume", 0)
            
            detailed_message = (
                f"{'âš ï¸' if current_level == 'warning' else 'ğŸš¨'} "
                f"í•˜ìˆ˜êµ¬ {'ì£¼ì˜ë³´' if current_level == 'warning' else 'ìœ„í—˜'}!\n"
                f"â€¢ ë§‰í˜ë¥ : {blockage_info}%\n"
                f"â€¢ íë¦„ ì œí•œ: {flow_restriction}\n"
                f"â€¢ ì¶•ì  ì“°ë ˆê¸°ëŸ‰: {garbage_volume}cmÂ³\n"
                f"â€¢ ìœ„í—˜ë„: {current_status['risk_score']:.1f}%"
            )
            
            alert = AlertData(
                level=current_level,
                message=detailed_message,
                timestamp=datetime.now(),
                risk_score=current_status['risk_score']
            )
            
            recent_alerts.appendleft(alert)
            current_status["alerts_today"] += 1
            logger.warning(f"ğŸš¨ ì•Œë¦¼ ë°œìƒ: {detailed_message}")
        
        # ìœ ì˜ë¯¸í•œ ë³€í™”ì‹œë§Œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        if significant_change:
            broadcast_data = {
                "type": "detection",
                "data": data.model_dump(),
                "status": current_status,
                "alert": alert.model_dump() if alert else None,
                "blockage_analysis": {
                    "blockage_percentage": current_status.get("blockage_percentage", 0),
                    "garbage_volume": current_status.get("garbage_volume", 0),
                    "flow_restriction": current_status.get("flow_restriction", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "accumulated_areas": current_status.get("accumulated_areas", 0)
                },
                "ai_analysis": current_status.get("ai_analysis", {})
            }
            
            await broadcast_to_clients(broadcast_data)
        
        return {
            "success": True,
            "duplicate": False,
            "significant_change": significant_change,
            "risk_score": current_status["risk_score"],
            "risk_level": current_level,
            "blockage_percentage": current_status.get("blockage_percentage", 0),
            "flow_restriction": current_status.get("flow_restriction", "ì•Œ ìˆ˜ ì—†ìŒ"),
            "alert_created": alert is not None
        }
        
    except Exception as e:
        logger.error(f"ê°ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status", response_model=StatusResponse)
async def get_status():
    """í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    return StatusResponse(**current_status)

@app.get("/blockage-analysis", response_model=BlockageAnalysis)
async def get_blockage_analysis():
    """í˜„ì¬ í•˜ìˆ˜êµ¬ ë§‰í˜ ë¶„ì„ ê²°ê³¼"""
    detections_list = list(recent_detections)
    return analyze_pipe_blockage(detections_list)

@app.get("/detections")
async def get_recent_detections(limit: int = 20):
    """ìµœê·¼ ê°ì§€ ê¸°ë¡ ì¡°íšŒ"""
    detections = list(recent_detections)[-limit:]
    return {
        "detections": [d.model_dump() for d in detections],
        "total": len(recent_detections),
        "limit": limit
    }

@app.get("/alerts")
async def get_recent_alerts(limit: int = 10):
    """ìµœê·¼ ì•Œë¦¼ ì¡°íšŒ"""
    alerts = list(recent_alerts)[:limit]
    return {
        "alerts": [a.model_dump() for a in alerts],
        "total": len(recent_alerts),
        "limit": limit
    }

@app.post("/reset")
async def reset_system():
    """ì‹œìŠ¤í…œ ìƒíƒœ ì´ˆê¸°í™”"""
    global current_status
    
    recent_detections.clear()
    recent_alerts.clear()
    
    current_status = {
        "risk_score": 0.0,
        "risk_level": "safe",
        "total_detections": 0,
        "last_detection": None,
        "alerts_today": 0,
        "pipe_status": "ì •ìƒ - ì›í™œí•œ íë¦„",
        "accumulation_rate": 0.0,
        "blockage_percentage": 0.0,
        "garbage_volume": 0.0,
        "flow_restriction": "ì—†ìŒ",
        "accumulated_areas": 0
    }
    
    await broadcast_to_clients({
        "type": "reset",
        "status": current_status
    })
    
    return {"success": True, "message": "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}

# ==================== ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ====================

from fastapi.responses import StreamingResponse

@app.get("/video_feed")
async def video_feed():
    """ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°"""
    def generate():
        while True:
            if current_frame is not None:
                ret, buffer = cv2.imencode('.jpg', current_frame)
                if ret:
                    frame_bytes = buffer.tobytes()
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
            else:
                # ê¸°ë³¸ ì´ë¯¸ì§€
                black_frame = np.zeros((480, 640, 3), dtype=np.uint8)
                cv2.putText(black_frame, 'Camera not active', (200, 240), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                ret, buffer = cv2.imencode('.jpg', black_frame)
                if ret:
                    frame_bytes = buffer.tobytes()
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
            
            import time
            time.sleep(0.033)  # ~30 FPS
    
    return StreamingResponse(generate(), media_type="multipart/x-mixed-replace; boundary=frame")

@app.post("/update_frame")
async def update_frame(frame_data: dict):
    """ì¹´ë©”ë¼ í”„ë ˆì„ ì—…ë°ì´íŠ¸"""
    global current_frame, camera_active
    
    try:
        frame_base64 = frame_data.get('frame')
        if frame_base64:
            frame_bytes = base64.b64decode(frame_base64)
            nparr = np.frombuffer(frame_bytes, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            
            if frame is not None:
                current_frame = frame
                camera_active = True
                return {"success": True, "message": "Frame updated"}
        
        return {"success": False, "message": "Invalid frame data"}
        
    except Exception as e:
        logger.error(f"í”„ë ˆì„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return {"success": False, "message": str(e)}

# ==================== WebSocket ====================

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°"""
    await websocket.accept()
    connected_clients.append(websocket)
    logger.info(f"ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°. ì´ ì—°ê²°: {len(connected_clients)}")
    
    try:
        # ì—°ê²° ì¦‰ì‹œ í˜„ì¬ ìƒíƒœ ì „ì†¡
        initial_data = {
            "type": "initial",
            "status": current_status,
            "recent_detections": [d.model_dump() for d in list(recent_detections)[-5:]],
            "recent_alerts": [a.model_dump() for a in list(recent_alerts)[:3]],
            "ai_analysis": current_status.get("ai_analysis", {})
        }
        await websocket.send_text(json.dumps(initial_data, default=str))
        
        # ì—°ê²° ìœ ì§€
        while True:
            message = await websocket.receive_text()
            if message == "ping":
                await websocket.send_text("pong")
                
    except WebSocketDisconnect:
        if websocket in connected_clients:
            connected_clients.remove(websocket)
        logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œ. ë‚¨ì€ ì—°ê²°: {len(connected_clients)}")
    except Exception as e:
        logger.error(f"WebSocket ì˜¤ë¥˜: {e}")
        if websocket in connected_clients:
            connected_clients.remove(websocket)

# ==================== í—¬ìŠ¤ì²´í¬ ====================

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "connected_clients": len(connected_clients),
        "camera_active": camera_active,
        "total_detections": len(recent_detections),
        "version": "2.0.0"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")